{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/graphllm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "import os \n",
    "import sys\n",
    "import numpy as np\n",
    "from xopen import xopen\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops\n",
    "\n",
    "def simMatrix(A: torch.tensor, B: torch.tensor) -> torch.tensor:\n",
    "    # Assume A and B are your input tensors of shape (N, d)\n",
    "    # Example: A = torch.randn(N, d)\n",
    "    #          B = torch.randn(N, d)\n",
    "\n",
    "    # Step 1: Normalize A and B\n",
    "    A_norm = A / A.norm(dim=1, keepdim=True)\n",
    "    B_norm = B / B.norm(dim=1, keepdim=True)\n",
    "\n",
    "    # Step 2: Compute the dot product\n",
    "    cosine_similarity_matrix = torch.mm(A_norm, B_norm.transpose(0, 1))\n",
    "\n",
    "    # The resulting cosine_similarity_matrix is of shape (N, N)\n",
    "    # and contains values in the range [-1, 1]\n",
    "    return cosine_similarity_matrix\n",
    "\n",
    "DATA_PATH = \"/home/ubuntu/proj/data/graph/node_cora\"\n",
    "DATA_NAME = \"text_graph_cora\" # \"text_graph_pubmed\" #\"text_graph_aids\" #\"text_graph_pubmed\" # # \n",
    "TRAIN_SPLIT_NAME = 'train_index'\n",
    "VALID_SPLIT_NAME = 'valid_index'\n",
    "TEST_SPLIT_NAME = 'test_index'\n",
    "\n",
    "with open(os.path.join(DATA_PATH, f\"{DATA_NAME}.pkl\"), 'rb') as f:\n",
    "    graph = pkl.load(f)\n",
    "with open(os.path.join(DATA_PATH, f\"{TRAIN_SPLIT_NAME}.pkl\"), 'rb') as f:\n",
    "    train_split = pkl.load(f)\n",
    "with open(os.path.join(DATA_PATH, f\"{VALID_SPLIT_NAME}.pkl\"), 'rb') as f:\n",
    "    valid_split = pkl.load(f)\n",
    "with open(os.path.join(DATA_PATH, f\"{TEST_SPLIT_NAME}.pkl\"), 'rb') as f:\n",
    "    test_split = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Embedding, Sequential, Linear, ModuleList, ReLU\n",
    "from typing import Callable, Union\n",
    "from torch_geometric.typing import OptPairTensor, Adj, OptTensor, Size\n",
    "from torch_geometric.nn import MessagePassing\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_channels_node, hidden_channels, output_channels, readout='add', num_layers=3):\n",
    "        super(MLP, self).__init__()\n",
    "        self.readout = readout\n",
    "        self.num_layers = num_layers\n",
    "        self.mlp = ModuleList()\n",
    "        block = Sequential(\n",
    "            Linear(input_channels_node, hidden_channels),\n",
    "            ReLU(),\n",
    "        )\n",
    "        self.mlp.append(block)\n",
    "        for _ in range(self.num_layers-2):\n",
    "            block = Sequential(\n",
    "                Linear(hidden_channels, hidden_channels),\n",
    "                ReLU(),\n",
    "            )\n",
    "            self.mlp.append(block)\n",
    "        block = Sequential(\n",
    "            Linear(hidden_channels, output_channels)\n",
    "        )\n",
    "        self.mlp.append(block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.mlp[i](x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_type='angle', pos_type='gcn', order=1, relevance_type='pos'\n",
      "Best Results: epoch=  3, train_score=0.972, valid_score=0.845, test_score=0.863\n",
      "encoder_type='angle', pos_type='gcn', order=1, relevance_type='neg'\n",
      "Best Results: epoch=  1, train_score=0.890, valid_score=0.847, test_score=0.865\n",
      "encoder_type='angle', pos_type='gcn', order=1, relevance_type='random_1'\n",
      "Best Results: epoch= 22, train_score=0.996, valid_score=0.845, test_score=0.878\n",
      "encoder_type='angle', pos_type='gcn', order=1, relevance_type='all'\n",
      "Best Results: epoch=  2, train_score=0.948, valid_score=0.852, test_score=0.876\n"
     ]
    }
   ],
   "source": [
    "encoder_type = 'angle'\n",
    "pos_type = \"gcn\"\n",
    "order = 1\n",
    "for relevance_type in ['pos','neg','random_1', 'all']:\n",
    "    SAVE_DIR = f\"/home/ubuntu/proj/code/axolotl_softprompt/data/cora/{relevance_type}\"\n",
    "    input_channels_node=768 if encoder_type == 'bert' else 1024\n",
    "    train_pos_tokens = torch.load(os.path.join(SAVE_DIR, f'train_{pos_type}_order{order}-{encoder_type}.pt'))\n",
    "    valid_pos_tokens = torch.load(os.path.join(SAVE_DIR, f'valid_{pos_type}_order{order}-{encoder_type}.pt'))\n",
    "    test_pos_tokens = torch.load(os.path.join(SAVE_DIR, f'test_{pos_type}_order{order}-{encoder_type}.pt'))\n",
    "\n",
    "    y_train, y_valid, y_test = graph.y[train_split], graph.y[valid_split], graph.y[test_split]\n",
    "    num_classes = len(torch.unique(y_train))\n",
    "\n",
    "    import torch\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "    # Create a TensorDataset\n",
    "    train_dataset = TensorDataset(train_pos_tokens, y_train)\n",
    "    valid_dataset = TensorDataset(valid_pos_tokens, y_valid)\n",
    "    test_dataset = TensorDataset(test_pos_tokens, y_test)\n",
    "\n",
    "    # create a DataLoader for batching\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size=64, shuffle=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    model = MLP(\n",
    "        input_channels_node=input_channels_node, \n",
    "        hidden_channels=1024, \n",
    "        output_channels=num_classes\n",
    "        )\n",
    "    device = torch.device('cuda:1')\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def train(dataloader):\n",
    "        loss_list = []\n",
    "        model.train()\n",
    "        for batch in dataloader:\n",
    "            batch_x, batch_y = batch\n",
    "            batch_x = batch_x.view(-1, input_channels_node)\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            output = model(batch_x)\n",
    "            loss = criterion(output, batch_y.long())\n",
    "            loss_list.append(float(loss))\n",
    "            loss.backward()\n",
    "            optimizer.step()  # Update parameters based on gradients.\n",
    "            optimizer.zero_grad()  # Clear gradients.\n",
    "        return np.mean(loss_list)\n",
    "\n",
    "    def test(dataloader):\n",
    "        y_true_list, y_pred_list = [], []\n",
    "        model.eval()\n",
    "        for batch in dataloader:\n",
    "            batch_x, batch_y = batch\n",
    "            batch_x = batch_x.view(-1, input_channels_node)\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            output = model(batch_x)\n",
    "            y_pred = output.argmax(dim=-1)\n",
    "            y_true_list.append(batch_y.detach().cpu().numpy().reshape(-1))\n",
    "            y_pred_list.append(y_pred.detach().cpu().numpy().reshape(-1))\n",
    "        y_true = np.concatenate(y_true_list)\n",
    "        y_pred = np.concatenate(y_pred_list)\n",
    "        return y_true, y_pred\n",
    "\n",
    "    num_epochs = 100\n",
    "\n",
    "    best_val_res = 0\n",
    "    best_res = {}\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        loss = train(train_dataloader)\n",
    "        y_true_train, y_pred_train = test(train_dataloader)\n",
    "        y_true_valid, y_pred_valid = test(valid_dataloader)\n",
    "        y_true_test, y_pred_test = test(test_dataloader)\n",
    "        train_score = np.mean(y_true_train == y_pred_train)\n",
    "        valid_score = np.mean(y_true_valid == y_pred_valid)\n",
    "        test_score = np.mean(y_true_test == y_pred_test)\n",
    "        if valid_score > best_val_res:\n",
    "            best_val_res = valid_score\n",
    "            best_res = {\n",
    "                'epoch': epoch,\n",
    "                'train_score': train_score,\n",
    "                'valid_score': valid_score,\n",
    "                'test_score': test_score,\n",
    "            }\n",
    "        #print(f\"{epoch=:3d}, {loss=:.3f}, {train_score=:.3f}, {valid_score=:.3f}, {test_score=:.3f}\")\n",
    "    epoch, train_score, valid_score, test_score = best_res['epoch'], best_res['train_score'], best_res['valid_score'], best_res['test_score']\n",
    "    print(f\"{encoder_type=}, {pos_type=}, {order=}, {relevance_type=}\")\n",
    "    print(f\"Best Results: {epoch=:3d}, {train_score=:.3f}, {valid_score=:.3f}, {test_score=:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
