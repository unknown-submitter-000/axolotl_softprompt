{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/graphllm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "import os \n",
    "import sys\n",
    "import numpy as np\n",
    "from xopen import xopen\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops\n",
    "\n",
    "def simMatrix(A: torch.tensor, B: torch.tensor) -> torch.tensor:\n",
    "    # Assume A and B are your input tensors of shape (N, d)\n",
    "    # Example: A = torch.randn(N, d)\n",
    "    #          B = torch.randn(N, d)\n",
    "\n",
    "    # Step 1: Normalize A and B\n",
    "    A_norm = A / A.norm(dim=1, keepdim=True)\n",
    "    B_norm = B / B.norm(dim=1, keepdim=True)\n",
    "\n",
    "    # Step 2: Compute the dot product\n",
    "    cosine_similarity_matrix = torch.mm(A_norm, B_norm.transpose(0, 1))\n",
    "\n",
    "    # The resulting cosine_similarity_matrix is of shape (N, N)\n",
    "    # and contains values in the range [-1, 1]\n",
    "    return cosine_similarity_matrix\n",
    "\n",
    "DATA_PATH = \"/home/ubuntu/proj/data/graph/node_cora\"\n",
    "DATA_NAME = \"text_graph_cora\" # \"text_graph_pubmed\" #\"text_graph_aids\" #\"text_graph_pubmed\" # # \n",
    "TRAIN_SPLIT_NAME = 'train_index'\n",
    "VALID_SPLIT_NAME = 'valid_index'\n",
    "TEST_SPLIT_NAME = 'test_index'\n",
    "\n",
    "with open(os.path.join(DATA_PATH, f\"{DATA_NAME}.pkl\"), 'rb') as f:\n",
    "    graph = pkl.load(f)\n",
    "with open(os.path.join(DATA_PATH, f\"{TRAIN_SPLIT_NAME}.pkl\"), 'rb') as f:\n",
    "    train_split = pkl.load(f)\n",
    "with open(os.path.join(DATA_PATH, f\"{VALID_SPLIT_NAME}.pkl\"), 'rb') as f:\n",
    "    valid_split = pkl.load(f)\n",
    "with open(os.path.join(DATA_PATH, f\"{TEST_SPLIT_NAME}.pkl\"), 'rb') as f:\n",
    "    test_split = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Embedding, Sequential, Linear, ModuleList, ReLU\n",
    "from typing import Callable, Union\n",
    "from torch_geometric.typing import OptPairTensor, Adj, OptTensor, Size\n",
    "from torch_geometric.nn import MessagePassing\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_channels_node, hidden_channels, output_channels, readout='add', num_layers=3):\n",
    "        super(MLP, self).__init__()\n",
    "        self.readout = readout\n",
    "        self.num_layers = num_layers\n",
    "        self.mlp = ModuleList()\n",
    "        block = Sequential(\n",
    "            Linear(input_channels_node, hidden_channels),\n",
    "            ReLU(),\n",
    "        )\n",
    "        self.mlp.append(block)\n",
    "        for _ in range(self.num_layers-2):\n",
    "            block = Sequential(\n",
    "                Linear(hidden_channels, hidden_channels),\n",
    "                ReLU(),\n",
    "            )\n",
    "            self.mlp.append(block)\n",
    "        block = Sequential(\n",
    "            Linear(hidden_channels, output_channels)\n",
    "        )\n",
    "        self.mlp.append(block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.mlp[i](x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1624it [00:00, 274626.04it/s]\n",
      "542it [00:00, 240766.02it/s]\n",
      "542it [00:00, 179665.91it/s]\n"
     ]
    }
   ],
   "source": [
    "SAVE_DIR = \"/home/ubuntu/proj/code/axolotl_softprompt/data/cora\"\n",
    "pos_type = \"textual\"\n",
    "order = 2\n",
    "train_pos_tokens = torch.load(os.path.join(SAVE_DIR, f'train_{pos_type}_order{order}.pt'))\n",
    "valid_pos_tokens = torch.load(os.path.join(SAVE_DIR, f'valid_{pos_type}_order{order}.pt'))\n",
    "test_pos_tokens = torch.load(os.path.join(SAVE_DIR, f'test_{pos_type}_order{order}.pt'))\n",
    "train_samples = []\n",
    "valid_samples = []\n",
    "test_samples = []\n",
    "with xopen(os.path.join(SAVE_DIR, 'train.jsonl')) as fin:\n",
    "    for i,line in tqdm(enumerate(fin)):\n",
    "        input_sample = json.loads(line)\n",
    "        train_samples.append(input_sample)\n",
    "with xopen(os.path.join(SAVE_DIR, 'valid.jsonl')) as fin:\n",
    "    for i,line in tqdm(enumerate(fin)):\n",
    "        input_sample = json.loads(line)\n",
    "        valid_samples.append(input_sample)\n",
    "with xopen(os.path.join(SAVE_DIR, 'test.jsonl')) as fin:\n",
    "    for i,line in tqdm(enumerate(fin)):\n",
    "        input_sample = json.loads(line)\n",
    "        test_samples.append(input_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_valid, y_test = graph.y[train_split], graph.y[valid_split], graph.y[test_split]\n",
    "#total_tokens = torch.cat([train_pos_tokens, test_pos_tokens])\n",
    "#y_total = torch.cat([y_train, y_test])\n",
    "#train_pos_tokens, test_pos_tokens = total_tokens[:len(total_tokens)//2], total_tokens[len(total_tokens)//2:]\n",
    "#y_train, y_test = y_total[:len(y_total)//2], y_total[len(y_total)//2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(torch.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Create a TensorDataset\n",
    "train_dataset = TensorDataset(train_pos_tokens, y_train)\n",
    "valid_dataset = TensorDataset(valid_pos_tokens, y_valid)\n",
    "test_dataset = TensorDataset(test_pos_tokens, y_test)\n",
    "\n",
    "# create a DataLoader for batching\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=64, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(input_channels_node=768, hidden_channels=1024, output_channels=num_classes)\n",
    "device = torch.device('cuda:0')\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader):\n",
    "    loss_list = []\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        batch_x, batch_y = batch\n",
    "        batch_x = batch_x.view(-1, 768)\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        output = model(batch_x)\n",
    "        loss = criterion(output, batch_y.long())\n",
    "        loss_list.append(float(loss))\n",
    "        loss.backward()\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "    return np.mean(loss_list)\n",
    "\n",
    "def test(dataloader):\n",
    "    y_true_list, y_pred_list = [], []\n",
    "    model.eval()\n",
    "    for batch in dataloader:\n",
    "        batch_x, batch_y = batch\n",
    "        batch_x = batch_x.view(-1, 768)\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        output = model(batch_x)\n",
    "        y_pred = output.argmax(dim=-1)\n",
    "        y_true_list.append(batch_y.detach().cpu().numpy().reshape(-1))\n",
    "        y_pred_list.append(y_pred.detach().cpu().numpy().reshape(-1))\n",
    "    y_true = np.concatenate(y_true_list)\n",
    "    y_pred = np.concatenate(y_pred_list)\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=  1, loss=1.717, train_score=0.415, valid_score=0.423, test_score=0.393\n",
      "epoch=  2, loss=1.245, train_score=0.683, valid_score=0.664, test_score=0.668\n",
      "epoch=  3, loss=0.996, train_score=0.727, valid_score=0.716, test_score=0.720\n",
      "epoch=  4, loss=0.801, train_score=0.754, valid_score=0.738, test_score=0.760\n",
      "epoch=  5, loss=0.710, train_score=0.780, valid_score=0.758, test_score=0.755\n",
      "epoch=  6, loss=0.641, train_score=0.807, valid_score=0.777, test_score=0.793\n",
      "epoch=  7, loss=0.583, train_score=0.799, valid_score=0.786, test_score=0.808\n",
      "epoch=  8, loss=0.550, train_score=0.813, valid_score=0.804, test_score=0.801\n",
      "epoch=  9, loss=0.535, train_score=0.848, valid_score=0.823, test_score=0.821\n",
      "epoch= 10, loss=0.524, train_score=0.821, valid_score=0.803, test_score=0.793\n",
      "epoch= 11, loss=0.532, train_score=0.832, valid_score=0.808, test_score=0.817\n",
      "epoch= 12, loss=0.512, train_score=0.843, valid_score=0.823, test_score=0.832\n",
      "epoch= 13, loss=0.492, train_score=0.850, valid_score=0.823, test_score=0.821\n",
      "epoch= 14, loss=0.468, train_score=0.853, valid_score=0.823, test_score=0.823\n",
      "epoch= 15, loss=0.459, train_score=0.860, valid_score=0.828, test_score=0.815\n",
      "epoch= 16, loss=0.475, train_score=0.851, valid_score=0.830, test_score=0.825\n",
      "epoch= 17, loss=0.442, train_score=0.870, valid_score=0.834, test_score=0.821\n",
      "epoch= 18, loss=0.436, train_score=0.866, valid_score=0.823, test_score=0.814\n",
      "epoch= 19, loss=0.428, train_score=0.871, valid_score=0.836, test_score=0.849\n",
      "epoch= 20, loss=0.427, train_score=0.858, valid_score=0.841, test_score=0.838\n",
      "epoch= 21, loss=0.414, train_score=0.873, valid_score=0.838, test_score=0.843\n",
      "epoch= 22, loss=0.424, train_score=0.844, valid_score=0.792, test_score=0.797\n",
      "epoch= 23, loss=0.436, train_score=0.880, valid_score=0.836, test_score=0.845\n",
      "epoch= 24, loss=0.436, train_score=0.876, valid_score=0.832, test_score=0.838\n",
      "epoch= 25, loss=0.390, train_score=0.887, valid_score=0.851, test_score=0.838\n",
      "epoch= 26, loss=0.404, train_score=0.876, valid_score=0.832, test_score=0.825\n",
      "epoch= 27, loss=0.378, train_score=0.900, valid_score=0.847, test_score=0.862\n",
      "epoch= 28, loss=0.347, train_score=0.888, valid_score=0.841, test_score=0.836\n",
      "epoch= 29, loss=0.393, train_score=0.868, valid_score=0.838, test_score=0.841\n",
      "epoch= 30, loss=0.399, train_score=0.877, valid_score=0.836, test_score=0.823\n",
      "epoch= 31, loss=0.360, train_score=0.900, valid_score=0.852, test_score=0.851\n",
      "epoch= 32, loss=0.350, train_score=0.898, valid_score=0.851, test_score=0.843\n",
      "epoch= 33, loss=0.327, train_score=0.903, valid_score=0.852, test_score=0.839\n",
      "epoch= 34, loss=0.330, train_score=0.903, valid_score=0.858, test_score=0.849\n",
      "epoch= 35, loss=0.325, train_score=0.893, valid_score=0.845, test_score=0.841\n",
      "epoch= 36, loss=0.335, train_score=0.882, valid_score=0.832, test_score=0.814\n",
      "epoch= 37, loss=0.333, train_score=0.897, valid_score=0.845, test_score=0.873\n",
      "epoch= 38, loss=0.340, train_score=0.894, valid_score=0.843, test_score=0.836\n",
      "epoch= 39, loss=0.366, train_score=0.893, valid_score=0.854, test_score=0.849\n",
      "epoch= 40, loss=0.342, train_score=0.903, valid_score=0.852, test_score=0.862\n",
      "epoch= 41, loss=0.318, train_score=0.887, valid_score=0.838, test_score=0.841\n",
      "epoch= 42, loss=0.329, train_score=0.903, valid_score=0.847, test_score=0.839\n",
      "epoch= 43, loss=0.325, train_score=0.910, valid_score=0.852, test_score=0.847\n",
      "epoch= 44, loss=0.311, train_score=0.906, valid_score=0.860, test_score=0.865\n",
      "epoch= 45, loss=0.304, train_score=0.901, valid_score=0.856, test_score=0.862\n",
      "epoch= 46, loss=0.299, train_score=0.908, valid_score=0.845, test_score=0.860\n",
      "epoch= 47, loss=0.315, train_score=0.912, valid_score=0.863, test_score=0.851\n",
      "epoch= 48, loss=0.288, train_score=0.904, valid_score=0.836, test_score=0.841\n",
      "epoch= 49, loss=0.331, train_score=0.903, valid_score=0.821, test_score=0.838\n",
      "epoch= 50, loss=0.286, train_score=0.894, valid_score=0.836, test_score=0.841\n",
      "epoch= 51, loss=0.311, train_score=0.911, valid_score=0.845, test_score=0.845\n",
      "epoch= 52, loss=0.310, train_score=0.900, valid_score=0.832, test_score=0.856\n",
      "epoch= 53, loss=0.303, train_score=0.892, valid_score=0.830, test_score=0.828\n",
      "epoch= 54, loss=0.324, train_score=0.898, valid_score=0.825, test_score=0.834\n",
      "epoch= 55, loss=0.325, train_score=0.897, valid_score=0.825, test_score=0.830\n",
      "epoch= 56, loss=0.309, train_score=0.901, valid_score=0.843, test_score=0.847\n",
      "epoch= 57, loss=0.295, train_score=0.908, valid_score=0.849, test_score=0.862\n",
      "epoch= 58, loss=0.296, train_score=0.904, valid_score=0.843, test_score=0.847\n",
      "epoch= 59, loss=0.299, train_score=0.922, valid_score=0.854, test_score=0.858\n",
      "epoch= 60, loss=0.274, train_score=0.881, valid_score=0.812, test_score=0.827\n",
      "epoch= 61, loss=0.306, train_score=0.910, valid_score=0.828, test_score=0.856\n",
      "epoch= 62, loss=0.270, train_score=0.884, valid_score=0.819, test_score=0.817\n",
      "epoch= 63, loss=0.312, train_score=0.915, valid_score=0.852, test_score=0.858\n",
      "epoch= 64, loss=0.279, train_score=0.903, valid_score=0.834, test_score=0.838\n",
      "epoch= 65, loss=0.254, train_score=0.922, valid_score=0.843, test_score=0.852\n",
      "epoch= 66, loss=0.262, train_score=0.914, valid_score=0.847, test_score=0.862\n",
      "epoch= 67, loss=0.283, train_score=0.900, valid_score=0.845, test_score=0.843\n",
      "epoch= 68, loss=0.261, train_score=0.925, valid_score=0.849, test_score=0.865\n",
      "epoch= 69, loss=0.255, train_score=0.924, valid_score=0.843, test_score=0.856\n",
      "epoch= 70, loss=0.253, train_score=0.919, valid_score=0.849, test_score=0.854\n",
      "epoch= 71, loss=0.273, train_score=0.879, valid_score=0.815, test_score=0.828\n",
      "epoch= 72, loss=0.253, train_score=0.932, valid_score=0.849, test_score=0.860\n",
      "epoch= 73, loss=0.236, train_score=0.936, valid_score=0.856, test_score=0.865\n",
      "epoch= 74, loss=0.248, train_score=0.914, valid_score=0.838, test_score=0.863\n",
      "epoch= 75, loss=0.237, train_score=0.910, valid_score=0.852, test_score=0.845\n",
      "epoch= 76, loss=0.247, train_score=0.916, valid_score=0.845, test_score=0.867\n",
      "epoch= 77, loss=0.263, train_score=0.900, valid_score=0.838, test_score=0.838\n",
      "epoch= 78, loss=0.260, train_score=0.920, valid_score=0.856, test_score=0.860\n",
      "epoch= 79, loss=0.247, train_score=0.926, valid_score=0.845, test_score=0.865\n",
      "epoch= 80, loss=0.221, train_score=0.901, valid_score=0.819, test_score=0.832\n",
      "epoch= 81, loss=0.259, train_score=0.916, valid_score=0.836, test_score=0.854\n",
      "epoch= 82, loss=0.270, train_score=0.931, valid_score=0.836, test_score=0.852\n",
      "epoch= 83, loss=0.246, train_score=0.938, valid_score=0.843, test_score=0.862\n",
      "epoch= 84, loss=0.233, train_score=0.925, valid_score=0.856, test_score=0.862\n",
      "epoch= 85, loss=0.238, train_score=0.935, valid_score=0.838, test_score=0.858\n",
      "epoch= 86, loss=0.252, train_score=0.932, valid_score=0.839, test_score=0.862\n",
      "epoch= 87, loss=0.235, train_score=0.933, valid_score=0.849, test_score=0.854\n",
      "epoch= 88, loss=0.221, train_score=0.927, valid_score=0.830, test_score=0.845\n",
      "epoch= 89, loss=0.219, train_score=0.919, valid_score=0.828, test_score=0.867\n",
      "epoch= 90, loss=0.244, train_score=0.907, valid_score=0.823, test_score=0.845\n",
      "epoch= 91, loss=0.222, train_score=0.919, valid_score=0.841, test_score=0.839\n",
      "epoch= 92, loss=0.218, train_score=0.936, valid_score=0.841, test_score=0.871\n",
      "epoch= 93, loss=0.228, train_score=0.933, valid_score=0.851, test_score=0.873\n",
      "epoch= 94, loss=0.208, train_score=0.945, valid_score=0.851, test_score=0.875\n",
      "epoch= 95, loss=0.200, train_score=0.887, valid_score=0.804, test_score=0.830\n",
      "epoch= 96, loss=0.280, train_score=0.922, valid_score=0.843, test_score=0.854\n",
      "epoch= 97, loss=0.214, train_score=0.946, valid_score=0.849, test_score=0.863\n",
      "epoch= 98, loss=0.192, train_score=0.927, valid_score=0.841, test_score=0.838\n",
      "epoch= 99, loss=0.218, train_score=0.943, valid_score=0.856, test_score=0.858\n",
      "epoch=100, loss=0.234, train_score=0.927, valid_score=0.830, test_score=0.851\n",
      "Best Results: epoch= 47, train_score=0.912, valid_score=0.863, test_score=0.851\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "best_val_res = 0\n",
    "best_res = {}\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    loss = train(train_dataloader)\n",
    "    y_true_train, y_pred_train = test(train_dataloader)\n",
    "    y_true_valid, y_pred_valid = test(valid_dataloader)\n",
    "    y_true_test, y_pred_test = test(test_dataloader)\n",
    "    train_score = np.mean(y_true_train == y_pred_train)\n",
    "    valid_score = np.mean(y_true_valid == y_pred_valid)\n",
    "    test_score = np.mean(y_true_test == y_pred_test)\n",
    "    if valid_score > best_val_res:\n",
    "        best_val_res = valid_score\n",
    "        best_res = {\n",
    "            'epoch': epoch,\n",
    "            'train_score': train_score,\n",
    "            'valid_score': valid_score,\n",
    "            'test_score': test_score,\n",
    "        }\n",
    "    print(f\"{epoch=:3d}, {loss=:.3f}, {train_score=:.3f}, {valid_score=:.3f}, {test_score=:.3f}\")\n",
    "epoch, train_score, valid_score, test_score = best_res['epoch'], best_res['train_score'], best_res['valid_score'], best_res['test_score']\n",
    "print(f\"Best Results: {epoch=:3d}, {train_score=:.3f}, {valid_score=:.3f}, {test_score=:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
