{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/graphllm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "import os \n",
    "import sys\n",
    "import numpy as np\n",
    "from xopen import xopen\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pandas as pd\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True)\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_embeddings(node_text):\n",
    "  model.eval().to(device)\n",
    "  marked_text = \"[CLS] \" + node_text + \" [SEP]\"\n",
    "  tokenized_text = tokenizer.tokenize(marked_text)\n",
    "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "  segments_ids = [1] * len(indexed_tokens)\n",
    "\n",
    "  seg_vecs = []\n",
    "  window_length, start = 510, 0\n",
    "  loop = True\n",
    "  while loop:\n",
    "    end = start + window_length\n",
    "    if end >= len(tokenized_text):\n",
    "        loop = False\n",
    "        end = len(tokenized_text)\n",
    "\n",
    "    indexed_tokens_chunk = indexed_tokens[start : end]\n",
    "    segments_ids_chunk = segments_ids[start : end]\n",
    "\n",
    "    indexed_tokens_chunk = [101] + indexed_tokens_chunk + [102]\n",
    "    segments_ids_chunk = [1] + segments_ids_chunk + [1]\n",
    "\n",
    "    tokens_tensor = torch.tensor([indexed_tokens_chunk]).to(device)\n",
    "    segments_tensors = torch.tensor([segments_ids_chunk]).to(device)\n",
    "    # Hidden embeddings: [n_layers, n_batches, n_tokens, n_features]\n",
    "    with torch.no_grad():\n",
    "      outputs = model(tokens_tensor, segments_tensors)\n",
    "      hidden_states = outputs[2]\n",
    "\n",
    "    seg_vecs.append(hidden_states[-2][0])\n",
    "    start += window_length\n",
    "\n",
    "  token_vecs = torch.cat(seg_vecs, dim=0)\n",
    "  sentence_embedding = torch.mean(token_vecs, dim=0).cpu()\n",
    "  return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_neighbors(node_text, neighbors, order):\n",
    "    PROMPTS_ROOT = os.getcwd()\n",
    "    prompt_filename = \"neighbors_assemble.prompt\"\n",
    "    with open(os.path.join(PROMPTS_ROOT, prompt_filename)) as f:\n",
    "        prompt_template = f.read().rstrip(\"\\n\")\n",
    "\n",
    "    num_neighbors = len(neighbors)\n",
    "    if num_neighbors == 0:\n",
    "        neighbor_text = \"[EMPTY]\"\n",
    "    else:\n",
    "        neighbor_text = []\n",
    "        for i in range(1, num_neighbors+1):\n",
    "            neighbor_text.append(f\"[Neighbor {i}] {neighbors[i-1]}\") \n",
    "        neighbor_text = \"\\n\".join(neighbor_text)\n",
    "\n",
    "    # Format the potential categories into strings\n",
    "    formatted_node_text = prompt_template.format(\n",
    "            node_description=node_text,\n",
    "            neighbor_text=neighbor_text,\n",
    "            order=order,\n",
    "            )\n",
    "    return formatted_node_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/home/ubuntu/proj/data/graph/node_pubmed\"\n",
    "DATA_NAME = \"text_graph_pubmed\" # \"text_graph_pubmed\" #\"text_graph_aids\" #\"text_graph_pubmed\" # # \n",
    "TRAIN_SPLIT_NAME = 'train_index'\n",
    "TEST_SPLIT_NAME = 'test_index'\n",
    "\n",
    "with open(os.path.join(DATA_PATH, f\"{DATA_NAME}.pkl\"), 'rb') as f:\n",
    "    graph = pkl.load(f)\n",
    "with open(os.path.join(DATA_PATH, f\"{TRAIN_SPLIT_NAME}.pkl\"), 'rb') as f:\n",
    "    train_split = pkl.load(f)\n",
    "with open(os.path.join(DATA_PATH, f\"{TEST_SPLIT_NAME}.pkl\"), 'rb') as f:\n",
    "    test_split = pkl.load(f)\n",
    "    \n",
    "text_nodes = graph.text_nodes\n",
    "edge_index = graph.edge_index\n",
    "k = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build 0-order textual-graph\n",
    "mapping_nodes_order = dict(zip(range(graph.num_nodes), text_nodes))\n",
    "mapping_edges = dict(zip(range(graph.num_nodes), [(edge_index[1][edge_index[0]==j]).numpy().tolist() for j in range(graph.num_nodes)]))\n",
    "\n",
    "# build higher order textual-graph\n",
    "all_levels_mapping = dict()\n",
    "all_levels_mapping[0] = mapping_nodes_order\n",
    "for order in range(k, 0, -1):\n",
    "    mapping_nodes_order = dict(\n",
    "        zip(\n",
    "            range(graph.num_nodes), \n",
    "            [assemble_neighbors(mapping_nodes_order[i],\n",
    "                                [mapping_nodes_order[neighbor] for neighbor in mapping_edges[i]],\n",
    "                                order\n",
    "                                ) for i in range(graph.num_nodes)]\n",
    "        )\n",
    "    )\n",
    "    all_levels_mapping[k-order+1] = mapping_nodes_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19717/19717 [02:44<00:00, 119.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# extract textual embeddings for each order\n",
    "all_levels_embedding = dict()\n",
    "for order in range(0, k+1):\n",
    "    current_level_embedding = dict()\n",
    "    for i in tqdm(range(graph.num_nodes)):\n",
    "        current_node_text = all_levels_mapping[order][i]\n",
    "        current_level_embedding[i] = bert_embeddings(current_node_text)\n",
    "    all_levels_embedding[order] = torch.stack([current_level_embedding[i] for i in range(graph.num_nodes)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for order in range(0, k+1):\n",
    "    torch.save(all_levels_embedding[order], os.path.join(DATA_PATH, f\"order-{order}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
