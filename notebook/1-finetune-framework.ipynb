{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/zheng_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/envs/zheng_env/lib/python3.11/site-packages/torch_geometric/typing.py:47: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /lib64/libm.so.6: version `GLIBC_2.29' not found (required by /opt/conda/envs/zheng_env/lib/python3.11/site-packages/libpyg.so)\n",
      "  warnings.warn(f\"An issue occurred while importing 'pyg-lib'. \"\n",
      "/opt/conda/envs/zheng_env/lib/python3.11/site-packages/torch_geometric/typing.py:90: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: /lib64/libm.so.6: version `GLIBC_2.27' not found (required by /opt/conda/envs/zheng_env/lib/python3.11/site-packages/torch_spline_conv/_basis_cuda.so)\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/zheng_env/lib/python3.11/site-packages/torch_geometric/typing.py:101: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /lib64/libm.so.6: version `GLIBC_2.29' not found (required by /opt/conda/envs/zheng_env/lib/python3.11/site-packages/torch_sparse/_metis_cuda.so)\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "import os \n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pandas as pd\n",
    "from torch_geometric.data import Data\n",
    "from accelerate import Accelerator\n",
    "\n",
    "\n",
    "sys.path.append(\"/home/ec2-user/proj/code/graphbert/src\")\n",
    "\n",
    "from utility.prompting import (\n",
    "    Item,\n",
    "    get_prompt_tuning_prompt\n",
    ")\n",
    "DATA_PATH = \"../data/text_graph\"\n",
    "DATA_NAME = \"text_graph_pubmed\" # \"text_graph_cora\"\n",
    "\n",
    "with open(os.path.join(DATA_PATH, f\"{DATA_NAME}.pkl\"), 'rb') as f:\n",
    "    graph = pkl.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(text_nodes=[19717], text_labels=[19717], y=[19717], x=[19717, 768], edge_index=[2, 44338])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create task prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = 'prompt_tuning'\n",
    "pubmed_item = Item(\n",
    "    desc = \"Question: Which category from the list that the paper most likely belong to?\",\n",
    "    categories = ['Diabetes Mellitus Type 1', 'Diabetes Mellitus Type 2','Diabetes Mellitus, Experimental'],\n",
    "    question = \"Given the keywords of a research paper, identify one category from a distinct list of research topics that you predict the paper will most likely belong to.\"\n",
    "    )\n",
    "hard_prompt = get_prompt_tuning_prompt(\n",
    "    task_name = task_name,\n",
    "    task_item = pubmed_item\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### USER: Question: Which category from the list that the paper most likely belong to? \n",
      "\n",
      "Belows are 3 potential categories to consider:\n",
      "Category [1](Diabetes Mellitus Type 1) \n",
      "Category [2](Diabetes Mellitus Type 2) \n",
      "Category [3](Diabetes Mellitus, Experimental) \n",
      "\n",
      "Given the keywords of a research paper, identify one category from a distinct list of research topics that you predict the paper will most likely belong to.\n",
      "### ASSISTANT:\n"
     ]
    }
   ],
   "source": [
    "print(hard_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here is how to use one GPU for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.41s/it]\n",
      "/opt/conda/envs/zheng_env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/zheng_env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load gpt-2 model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, default_data_collator, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_name_or_path = \"/home/ec2-user/proj/llm_models/vicuna-7b-v1.5\"\n",
    "tokenizer_name_or_path = \"/home/ec2-user/proj/llm_models/vicuna-7b-v1.5\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 17,581,056 || all params: 6,755,996,672 || trainable%: 0.26022890261127707\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# build our graph prompt tuning model\n",
    "from tuner import GraphPeftType, GraphPromptTuningConfig\n",
    "from mapping import get_peft_graph_model\n",
    "from peft import TaskType\n",
    "peft_config = GraphPromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    input_embedding_dim=768,\n",
    "    num_virtual_tokens=4,\n",
    "    encoder_hidden_size=1024,\n",
    "    embed_projection=True\n",
    ")\n",
    "model = get_peft_graph_model(model, peft_config)\n",
    "print(model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = model.prompt_encoder['default'].transform.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[ 1.5565e-02, -8.9188e-03, -1.8593e-02,  ..., -1.4404e-03,\n",
       "                       -3.3002e-02, -1.6373e-02],\n",
       "                      [ 2.2331e-02,  3.5551e-02,  1.4876e-02,  ..., -3.3919e-03,\n",
       "                        1.7511e-02,  1.0691e-02],\n",
       "                      [ 3.4553e-02,  2.1867e-03, -1.0429e-02,  ...,  1.8481e-02,\n",
       "                       -2.3085e-02,  5.7828e-03],\n",
       "                      ...,\n",
       "                      [-5.5246e-03, -3.5043e-02, -1.2255e-02,  ...,  2.7078e-02,\n",
       "                       -3.2238e-02,  2.6146e-02],\n",
       "                      [ 1.9129e-02,  7.3476e-05,  1.8694e-02,  ..., -3.7632e-03,\n",
       "                       -1.6735e-02, -1.7149e-02],\n",
       "                      [ 2.6266e-02,  1.8581e-02,  3.0920e-02,  ..., -3.3916e-02,\n",
       "                       -4.3927e-03, -2.8788e-02]])),\n",
       "             ('0.bias',\n",
       "              tensor([-0.0221,  0.0203,  0.0139,  ...,  0.0120, -0.0332,  0.0146])),\n",
       "             ('2.weight',\n",
       "              tensor([[-0.0166,  0.0241,  0.0105,  ...,  0.0025,  0.0144,  0.0186],\n",
       "                      [-0.0284,  0.0008,  0.0079,  ...,  0.0003, -0.0070,  0.0085],\n",
       "                      [ 0.0272, -0.0074, -0.0301,  ..., -0.0156,  0.0010, -0.0219],\n",
       "                      ...,\n",
       "                      [-0.0144, -0.0239,  0.0252,  ..., -0.0193,  0.0255,  0.0007],\n",
       "                      [-0.0217, -0.0171,  0.0273,  ...,  0.0002, -0.0080,  0.0053],\n",
       "                      [ 0.0055,  0.0228,  0.0049,  ...,  0.0285,  0.0181,  0.0144]])),\n",
       "             ('2.bias',\n",
       "              tensor([ 0.0093,  0.0073,  0.0266,  ...,  0.0279,  0.0039, -0.0181]))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "data = Dataset.from_dict(\n",
    "    {   \n",
    "        'embeds': graph.x,\n",
    "        'labels': graph.text_labels,\n",
    "    }\n",
    ")\n",
    "\n",
    "# settings \n",
    "max_length = 128\n",
    "lr = 5e-3\n",
    "num_epochs = 2\n",
    "batch_size = 1\n",
    "\n",
    "# Split data into train and test with 50% for train and 50% for test\n",
    "split_data = data.train_test_split(test_size=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    batch_size = len(examples['labels'])\n",
    "    embeds = examples['embeds']\n",
    "    targets = examples['labels']\n",
    "\n",
    "    # tokenize task prompt and targets\n",
    "    prompt = tokenizer(hard_prompt)\n",
    "    labels = tokenizer(targets) \n",
    "    labels['labels'] = []\n",
    "    labels['prompt_tokens'] = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # extract graph embedding, prompt ids, and target ids\n",
    "        graph_embeds = embeds[i]\n",
    "        prompt_ids = prompt[\"input_ids\"]\n",
    "        label_input_ids = labels[\"input_ids\"][i] + [tokenizer.pad_token_id]\n",
    "\n",
    "        sample_input_ids = prompt_ids + label_input_ids\n",
    "        attention_mask = [1] * len(labels[\"input_ids\"][i])\n",
    "\n",
    "        model_input_ids = [tokenizer.pad_token_id] * (\n",
    "            max_length - len(sample_input_ids)\n",
    "        ) + sample_input_ids\n",
    "        attention_mask = [0] * (max_length - len(sample_input_ids)) + [1] * len(sample_input_ids)\n",
    "        label_ids = [-100] * (max_length - len(label_input_ids)) + label_input_ids\n",
    "\n",
    "        labels[\"input_ids\"][i] = torch.tensor(model_input_ids[:max_length])\n",
    "        labels[\"attention_mask\"][i] = torch.tensor(attention_mask[:max_length])\n",
    "        labels[\"labels\"].append(torch.tensor(label_ids[:max_length]))\n",
    "        labels['prompt_tokens'].append(torch.tensor(graph_embeds))\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prepare dataset for Graph input (num_proc=16):   0%|          | 0/9858 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prepare dataset for Graph input (num_proc=16): 100%|██████████| 9858/9858 [00:04<00:00, 2336.10 examples/s]\n",
      "Prepare dataset for Graph input (num_proc=16): 100%|██████████| 9859/9859 [00:04<00:00, 2387.75 examples/s]\n"
     ]
    }
   ],
   "source": [
    "processed_datasets = split_data.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=16,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Prepare dataset for Graph input\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    processed_datasets['train'], shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True\n",
    ")\n",
    "test_dataloader = DataLoader(processed_datasets['test'], collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator()\n",
    "\n",
    "model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9858/9858 [45:23<00:00,  3.62it/s]\n",
      "100%|██████████| 9859/9859 [23:21<00:00,  7.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0: train_ppl=tensor(1.7064, device='cuda:0') train_epoch_loss=tensor(0.5344, device='cuda:0') eval_ppl=tensor(1.6939, device='cuda:0') eval_epoch_loss=tensor(0.5270, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9858/9858 [45:23<00:00,  3.62it/s]\n",
      "100%|██████████| 9859/9859 [23:20<00:00,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1: train_ppl=tensor(1.6454, device='cuda:0') train_epoch_loss=tensor(0.4980, device='cuda:0') eval_ppl=tensor(1.6216, device='cuda:0') eval_epoch_loss=tensor(0.4834, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = accelerator.device\n",
    "#model = model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        #batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        batch = {k: v for k, v in batch.items()}\n",
    "        outputs = model(input_ids=batch[\"input_ids\"], prompt_tokens=batch['embeds'], attention_mask=batch[\"attention_mask\"], labels=batch['labels'])\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.detach().float()\n",
    "        #loss.backward()\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    eval_preds = []\n",
    "    for step, batch in enumerate(tqdm(test_dataloader)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=batch[\"input_ids\"], prompt_tokens=batch['embeds'], attention_mask=batch[\"attention_mask\"], labels=batch['labels'])\n",
    "        loss = outputs.loss\n",
    "        eval_loss += loss.detach().float()\n",
    "        eval_preds.extend(\n",
    "            tokenizer.batch_decode(torch.argmax(outputs.logits, -1).detach().cpu().numpy(), skip_special_tokens=True)\n",
    "        )\n",
    "\n",
    "    eval_epoch_loss = eval_loss / len(test_dataloader)\n",
    "    eval_ppl = torch.exp(eval_epoch_loss)\n",
    "    train_epoch_loss = total_loss / len(train_dataloader)\n",
    "    train_ppl = torch.exp(train_epoch_loss)\n",
    "    #print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=}\")\n",
    "    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9859 [00:00<?, ?it/s]/opt/conda/envs/zheng_env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/zheng_env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/proj/code/graphbert/src/peft_graph_model.py:216: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n",
      "  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n",
      "100%|██████████| 9859/9859 [00:22<00:00, 436.23it/s] \n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(hard_prompt,return_tensors=\"pt\")\n",
    "device = 'cuda:0'\n",
    "generated_answer = []\n",
    "original_answer = []\n",
    "for step, batch in enumerate(tqdm(split_data['test'])):\n",
    "    if step>10:\n",
    "        continue\n",
    "    with torch.no_grad():\n",
    "        input_ids = inputs[\"input_ids\"].to(device).view(1,-1)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(device).view(1,-1)\n",
    "        embeds = torch.tensor(batch['embeds'], device=input_ids.device).view(1,-1)\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids, \n",
    "            prompt_tokens=embeds, \n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=64\n",
    "            ).detach().cpu()\n",
    "        generated_answer.append(tokenizer.decode(outputs[0])) \n",
    "        original_answer.append(batch['labels']) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "correct_num = 0\n",
    "total_num = 0\n",
    "generated_results = [item.split('###')[-1] for item in generated_answer]\n",
    "for prediction, groundtruth in zip(generated_results, original_answer):\n",
    "    if groundtruth in prediction:\n",
    "        correct_num += 1\n",
    "    total_num += 1\n",
    "print(f\"Accuracy: {correct_num/total_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ASSISTANT:, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type',\n",
       " ' ASSISTANT:, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type, Type',\n",
       " ' ASSISTANT: Type12 Type12 Type12 Type12 Type12 Type112 Type112 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type1',\n",
       " ' ASSISTANT: Type12 Type Type Type12 Type12 Type12 Type12 Type112 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type',\n",
       " ' ASSISTANT: Type2 Type Type Type Type Type2 Type12 Type12 Type12 Type112 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type2 Type12 Type12 Type12 Type12 Type2 Type12 Type12',\n",
       " ' ASSISTANT: Type2 Type Type Type Type12 Type12 Type12 Type12 Type112 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type',\n",
       " ' ASSISTANT: Type12 Type12 Type12 Type12 Type12 Type112 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type12',\n",
       " ' ASSISTANT: Type2 Type Type Type Type12 Type12 Type12 Type12 Type112 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type',\n",
       " ' ASSISTANT: Type2 Type Type Type Type12 Type12 Type12 Type12 Type112 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type',\n",
       " ' ASSISTANT: Type2 Type Type Type Type Type2 Type12 Type12 Type12 Type112 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type2 Type12 Type12 Type12 Type12 Type2 Type12 Type12',\n",
       " ' ASSISTANT: Type2 Type Type Type Type Type2 Type12 Type12 Type12 Type112 Type12 Type12 Type12 Type12 Type12 Type12 Type12 Type2 Type12 Type12 Type12 Type12 Type2 Type12 Type12']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9858/9858 [00:12<00:00, 766.93it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "    #batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    batch = {k: v for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['embeds', 'labels', 'input_ids', 'attention_mask', 'prompt_tokens'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zheng_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
