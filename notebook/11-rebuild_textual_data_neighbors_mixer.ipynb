{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/graphllm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "import os \n",
    "import sys\n",
    "import numpy as np\n",
    "from xopen import xopen\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops\n",
    "\n",
    "def simMatrix(A: torch.tensor, B: torch.tensor) -> torch.tensor:\n",
    "    # Assume A and B are your input tensors of shape (N, d)\n",
    "    # Example: A = torch.randn(N, d)\n",
    "    #          B = torch.randn(N, d)\n",
    "\n",
    "    # Step 1: Normalize A and B\n",
    "    A_norm = A / A.norm(dim=1, keepdim=True)\n",
    "    B_norm = B / B.norm(dim=1, keepdim=True)\n",
    "\n",
    "    # Step 2: Compute the dot product\n",
    "    cosine_similarity_matrix = torch.mm(A_norm, B_norm.transpose(0, 1))\n",
    "\n",
    "    # The resulting cosine_similarity_matrix is of shape (N, N)\n",
    "    # and contains values in the range [-1, 1]\n",
    "    return cosine_similarity_matrix\n",
    "\n",
    "DATA_PATH = \"/home/ubuntu/proj/data/graph/node_sports\"\n",
    "DATA_NAME = \"text_graph_sports\" # \"text_graph_pubmed\" #\"text_graph_aids\" #\"text_graph_pubmed\" # # \n",
    "\n",
    "with open(os.path.join(DATA_PATH, f\"{DATA_NAME}.pkl\"), 'rb') as f:\n",
    "    graph = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True)\n",
    "device = torch.device(\"cuda:0\")\n",
    "def bert_embeddings(node_text):\n",
    "  model.eval().to(device)\n",
    "  marked_text = \"[CLS] \" + node_text + \" [SEP]\"\n",
    "  tokenized_text = tokenizer.tokenize(marked_text)\n",
    "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "  segments_ids = [1] * len(indexed_tokens)\n",
    "\n",
    "  seg_vecs = []\n",
    "  window_length, start = 510, 0\n",
    "  loop = True\n",
    "  while loop:\n",
    "    end = start + window_length\n",
    "    if end >= len(tokenized_text):\n",
    "        loop = False\n",
    "        end = len(tokenized_text)\n",
    "\n",
    "    indexed_tokens_chunk = indexed_tokens[start : end]\n",
    "    segments_ids_chunk = segments_ids[start : end]\n",
    "\n",
    "    indexed_tokens_chunk = [101] + indexed_tokens_chunk + [102]\n",
    "    segments_ids_chunk = [1] + segments_ids_chunk + [1]\n",
    "\n",
    "    tokens_tensor = torch.tensor([indexed_tokens_chunk]).to(device)\n",
    "    segments_tensors = torch.tensor([segments_ids_chunk]).to(device)\n",
    "    # Hidden embeddings: [n_layers, n_batches, n_tokens, n_features]\n",
    "    with torch.no_grad():\n",
    "      outputs = model(tokens_tensor, segments_tensors)\n",
    "      hidden_states = outputs[2]\n",
    "\n",
    "    seg_vecs.append(hidden_states[-2][0])\n",
    "    start += window_length\n",
    "\n",
    "  token_vecs = torch.cat(seg_vecs, dim=0)\n",
    "  sentence_embedding = torch.mean(token_vecs, dim=0).cpu()\n",
    "  return sentence_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from angle_emb import AnglE\n",
    "\n",
    "#angle = AnglE.from_pretrained('WhereIsAI/UAE-Large-V1', pooling_strategy='cls').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/173055 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 173055/173055 [22:41<00:00, 127.13it/s]\n"
     ]
    }
   ],
   "source": [
    "all_inputs = graph.text_nodes\n",
    "all_embeddings = []\n",
    "for inputs in tqdm(all_inputs):\n",
    "    #vec = angle.encode(inputs, to_numpy=True)\n",
    "    vec = bert_embeddings(inputs).numpy().reshape(1,-1)\n",
    "    #print(vec)\n",
    "    all_embeddings.append(copy.deepcopy(vec))\n",
    "all_embeddings = np.concatenate(all_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute pairwise similarity\n",
    "# Normalize the vectors to have unit norm\n",
    "all_embeddings_normalized = all_embeddings / np.linalg.norm(all_embeddings, axis=1)[:, np.newaxis]\n",
    "\n",
    "# Compute the cosine similarity matrix\n",
    "similarity_matrix = np.dot(all_embeddings_normalized, all_embeddings_normalized.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_dense_adj\n",
    "adj = to_dense_adj(graph.edge_index)[0].numpy()\n",
    "similarity_matrix = adj * similarity_matrix # filter out the similarity score with no connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "sparse_matrix = csr_matrix(similarity_matrix)\n",
    "\n",
    "# Initialize a list to store the ranking indices for each row\n",
    "ranking_indices_per_row = []\n",
    "\n",
    "for i in range(sparse_matrix.shape[0]):\n",
    "    row = sparse_matrix[i].toarray().ravel()  # Convert the sparse row to a dense format\n",
    "    nonzero_indices = row.nonzero()[0]  # Find indices of non-zero elements\n",
    "    sorted_indices = nonzero_indices[np.argsort(row[nonzero_indices])][::-1]  # Sort indices by value, in descending order\n",
    "    ranking_indices_per_row.append(sorted_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_neighbors(node_text, neighbors, order, relevance='pos'):\n",
    "    PROMPTS_ROOT = os.getcwd()\n",
    "    if relevance == 'pos':\n",
    "        prompt_filename = \"neighbors_assemble_relevance.prompt\"\n",
    "    elif relevance == 'neg':\n",
    "        prompt_filename = \"neighbors_assemble_anti-relevance.prompt\"\n",
    "    elif relevance == 'random':\n",
    "        prompt_filename = \"neighbors_assemble_random-relevance.prompt\"\n",
    "    with open(os.path.join(PROMPTS_ROOT, prompt_filename)) as f:\n",
    "        prompt_template = f.read().rstrip(\"\\n\")\n",
    "\n",
    "    num_neighbors = len(neighbors)\n",
    "    if num_neighbors == 0:\n",
    "        neighbor_text = \"[EMPTY]\"\n",
    "    else:\n",
    "        neighbor_text = []\n",
    "        for i in range(1, num_neighbors+1):\n",
    "            neighbor_text.append(f\"[Neighbor {i}] {neighbors[i-1]}\") \n",
    "        neighbor_text = \"\\n\".join(neighbor_text)\n",
    "\n",
    "    # Format the potential categories into strings\n",
    "    formatted_node_text = prompt_template.format(\n",
    "            node_description=node_text,\n",
    "            neighbor_text=neighbor_text,\n",
    "            order=order,\n",
    "            )\n",
    "    return formatted_node_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/graphllm/lib/python3.11/site-packages/torch_geometric/data/storage.py:327: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'text_nodes', 'text_node_labels', 'edge_index', 'y'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
      "  warnings.warn(\n",
      "100%|██████████| 173055/173055 [23:12<00:00, 124.30it/s]\n",
      "  8%|▊         | 14661/173055 [04:34<1:21:04, 32.56it/s]"
     ]
    }
   ],
   "source": [
    "for relevance_type in ['neg','pos','random_1','random_2','random_3']:\n",
    "    # build 0-order textual-graph\n",
    "    text_nodes = graph.text_nodes\n",
    "    edge_index = graph.edge_index\n",
    "    k = 2\n",
    "\n",
    "    mapping_nodes_order = dict(zip(range(graph.num_nodes), graph.text_nodes))\n",
    "    mapping_edges = dict(zip(range(graph.num_nodes), ranking_indices_per_row))\n",
    "\n",
    "    # build higher order textual-graph in postive ordering\n",
    "    all_levels_mapping = dict()\n",
    "    all_levels_mapping[0] = mapping_nodes_order\n",
    "    for order in range(0, k+1):\n",
    "        if order > 0:\n",
    "            if relevance_type == 'pos':\n",
    "                mapping_nodes_order = dict(\n",
    "                    zip(\n",
    "                        range(graph.num_nodes), \n",
    "                        [assemble_neighbors(mapping_nodes_order[i],\n",
    "                                            [mapping_nodes_order[neighbor] for neighbor in mapping_edges[i]],\n",
    "                                            order, \n",
    "                                            relevance='pos'\n",
    "                                            ) for i in range(graph.num_nodes)]\n",
    "                    )\n",
    "                )\n",
    "            elif relevance_type == 'neg':\n",
    "                mapping_nodes_order = dict(\n",
    "                    zip(\n",
    "                        range(graph.num_nodes), \n",
    "                        [assemble_neighbors(mapping_nodes_order[i],\n",
    "                                            [mapping_nodes_order[neighbor] for neighbor in mapping_edges[i][::-1]],\n",
    "                                            order, \n",
    "                                            relevance='neg'\n",
    "                                            ) for i in range(graph.num_nodes)]\n",
    "                    )\n",
    "                )\n",
    "            elif relevance_type.startswith('random'):\n",
    "                mapping_nodes_order = dict(\n",
    "                    zip(\n",
    "                        range(graph.num_nodes), \n",
    "                        [assemble_neighbors(mapping_nodes_order[i],\n",
    "                                            [mapping_nodes_order[neighbor] for neighbor in np.random.permutation(mapping_edges[i])],\n",
    "                                            order, \n",
    "                                            relevance='random'\n",
    "                                            ) for i in range(graph.num_nodes)]\n",
    "                    )\n",
    "                )\n",
    "        all_levels_mapping[order] = mapping_nodes_order\n",
    "\n",
    "    # extract textual embeddings for each order\n",
    "    all_levels_embedding = dict()\n",
    "    for order in range(0, k+1):\n",
    "        current_level_embedding = dict()\n",
    "        for i in tqdm(range(graph.num_nodes)):\n",
    "            current_node_text = all_levels_mapping[order][i]\n",
    "            #current_level_embedding[i] = torch.tensor(angle.encode(current_node_text), dtype=torch.float)\n",
    "            vec = copy.deepcopy(bert_embeddings(current_node_text).numpy().reshape(1,-1))\n",
    "            current_level_embedding[i] = torch.tensor(vec, dtype=torch.float)\n",
    "        all_levels_embedding[order] = torch.stack([current_level_embedding[i] for i in range(graph.num_nodes)])\n",
    " \n",
    "    for order in range(0, k+1):\n",
    "        if not os.path.exists(os.path.join(DATA_PATH, relevance_type)):\n",
    "            os.makedirs(os.path.join(DATA_PATH, relevance_type))\n",
    "        torch.save(all_levels_embedding[order], os.path.join(DATA_PATH, relevance_type, f\"order-{order}-bert.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([Descriptions]: Title: The megaprior heuristic for discovering protein sequence patterns  \n",
      "Abstract: Several computer algorithms for discovering patterns in groups of protein sequences are in use that are based on fitting the parameters of a statistical model to a group of related sequences. These include hidden Markov model (HMM) algorithms for multiple sequence alignment, and the MEME and Gibbs sampler algorithms for discovering motifs. These algorithms are sometimes prone to producing models that are incorrect because two or more patterns have been combined. The statistical model produced in this situation is a convex combination (weighted average) of two or more different models. This paper presents a solution to the problem of convex combinations in the form of a heuristic based on using extremely low variance Dirichlet mixture priors as part of the statistical model. This heuristic, which we call the megaprior heuristic, increases the strength (i.e., decreases the variance) of the prior in proportion to the size of the sequence dataset. This causes each column in the final model to strongly resemble the mean of a single component of the prior, regardless of the size of the dataset. We describe the cause of the convex combination problem, analyze it mathematically, motivate and describe the implementation of the megaprior heuristic, and show how it can effectively eliminate the problem of convex combinations in protein sequence pattern discovery. . \n",
      "[Order-1 Neighbors in the ordering of low-to-high relevance]: \n",
      "[Neighbor 1] Title: Meta-MEME: Motif-based Hidden Markov Models of Protein Families  \n",
      "Abstract: In a previous paper [SM95], we showed how finite automata could be used to define objective functions for assessing the quality of an alignment of two (or more) sequences. In this paper, we show some results of using such cost functions. We also show how to extend Hischberg's linear space algorithm [Hir75] to this setting, thus generalizing a result of Myers and Miller [MM88b]. \n",
      "[Neighbor 2] Title: Homology Detection via Family Pairwise Search a straightforward generalization of pairwise sequence comparison algorithms to\n",
      "Abstract: The function of an unknown biological sequence can often be accurately inferred by identifying sequences homologous to the original sequence. Given a query set of known homologs, there exist at least three general classes of techniques for finding additional homologs: pairwise sequence comparisons, motif analysis, and hidden Markov modeling. Pairwise sequence comparisons are typically employed when only a single query sequence is known. Hidden Markov models (HMMs), on the other hand, are usually trained with sets of more than 100 sequences. Motif-based methods fall in between these two extremes. \n",
      "[Neighbor 3] Title: Minimum-Risk Profiles of Protein Families Based on Statistical Decision Theory  \n",
      "Abstract: Statistical decision theory provides a principled way to estimate amino acid frequencies in conserved positions of a protein family. The goal is to minimize the risk function, or the expected squared-error distance between the estimates and the true population frequencies. The minimum-risk estimates are obtained by adding an optimal number of pseudocounts to the observed data. Two formulas are presented, one for pseudocounts based on marginal amino acid frequencies and one for pseudocounts based on the observed data. Experimental results show that profiles constructed using minimal-risk estimates are more discriminating than those constructed using existing methods.\n",
      "[Neighbor 4] Title: Hidden Markov Models in Computational Biology: Applications to Protein Modeling UCSC-CRL-93-32 Keywords: Hidden Markov Models,\n",
      "Abstract: Hidden Markov Models (HMMs) are applied to the problems of statistical modeling, database searching and multiple sequence alignment of protein families and protein domains. These methods are demonstrated on the globin family, the protein kinase catalytic domain, and the EF-hand calcium binding motif. In each case the parameters of an HMM are estimated from a training set of unaligned sequences. After the HMM is built, it is used to obtain a multiple alignment of all the training sequences. It is also used to search the SWISS-PROT 22 database for other sequences that are members of the given protein family, or contain the given domain. The HMM produces multiple alignments of good quality that agree closely with the alignments produced by programs that incorporate three-dimensional structural information. When employed in discrimination tests (by examining how closely the sequences in a database fit the globin, kinase and EF-hand HMMs), the HMM is able to distinguish members of these families from non-members with a high degree of accuracy. Both the HMM and PRO-FILESEARCH (a technique used to search for relationships between a protein sequence and multiply aligned sequences) perform better in these tests than PROSITE (a dictionary of sites and patterns in proteins). The HMM appears to have a slight advantage \n",
      "[Neighbor 5] Title: Using Dirichlet Mixture Priors to Derive Hidden Markov Models for Protein Families  \n",
      "Abstract: A Bayesian method for estimating the amino acid distributions in the states of a hidden Markov model (HMM) for a protein family or the columns of a multiple alignment of that family is introduced. This method uses Dirichlet mixture densities as priors over amino acid distributions. These mixture densities are determined from examination of previously constructed HMMs or multiple alignments. It is shown that this Bayesian method can improve the quality of HMMs produced from small training sets. Specific experiments on the EF-hand motif are reported, for which these priors are shown to produce HMMs with higher likelihood on unseen data, and fewer false positives and false negatives in a database search task. \n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(all_levels_mapping[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
