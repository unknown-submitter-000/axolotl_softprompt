{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/graphllm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "import os \n",
    "import sys\n",
    "import numpy as np\n",
    "from xopen import xopen\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops\n",
    "\n",
    "def simMatrix(A: torch.tensor, B: torch.tensor) -> torch.tensor:\n",
    "    # Assume A and B are your input tensors of shape (N, d)\n",
    "    # Example: A = torch.randn(N, d)\n",
    "    #          B = torch.randn(N, d)\n",
    "\n",
    "    # Step 1: Normalize A and B\n",
    "    A_norm = A / A.norm(dim=1, keepdim=True)\n",
    "    B_norm = B / B.norm(dim=1, keepdim=True)\n",
    "\n",
    "    # Step 2: Compute the dot product\n",
    "    cosine_similarity_matrix = torch.mm(A_norm, B_norm.transpose(0, 1))\n",
    "\n",
    "    # The resulting cosine_similarity_matrix is of shape (N, N)\n",
    "    # and contains values in the range [-1, 1]\n",
    "    return cosine_similarity_matrix\n",
    "\n",
    "DATA_PATH = \"/home/ubuntu/proj/data/graph/node_pubmed\"\n",
    "DATA_NAME = \"text_graph_pubmed\" # \"text_graph_pubmed\" #\"text_graph_aids\" #\"text_graph_pubmed\" # # \n",
    "TRAIN_SPLIT_NAME = 'train_index'\n",
    "TEST_SPLIT_NAME = 'test_index'\n",
    "\n",
    "with open(os.path.join(DATA_PATH, f\"{DATA_NAME}.pkl\"), 'rb') as f:\n",
    "    graph = pkl.load(f)\n",
    "with open(os.path.join(DATA_PATH, f\"{TRAIN_SPLIT_NAME}.pkl\"), 'rb') as f:\n",
    "    train_split = pkl.load(f)\n",
    "with open(os.path.join(DATA_PATH, f\"{TEST_SPLIT_NAME}.pkl\"), 'rb') as f:\n",
    "    test_split = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Embedding, Sequential, Linear, ModuleList, ReLU\n",
    "from typing import Callable, Union\n",
    "from torch_geometric.typing import OptPairTensor, Adj, OptTensor, Size\n",
    "from torch_geometric.nn import MessagePassing\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_channels_node, hidden_channels, output_channels, readout='add', num_layers=3):\n",
    "        super(MLP, self).__init__()\n",
    "        self.readout = readout\n",
    "        self.num_layers = num_layers\n",
    "        self.mlp = ModuleList()\n",
    "        block = Sequential(\n",
    "            Linear(input_channels_node, hidden_channels),\n",
    "            ReLU(),\n",
    "        )\n",
    "        self.mlp.append(block)\n",
    "        for _ in range(self.num_layers-2):\n",
    "            block = Sequential(\n",
    "                Linear(hidden_channels, hidden_channels),\n",
    "                ReLU(),\n",
    "            )\n",
    "            self.mlp.append(block)\n",
    "        block = Sequential(\n",
    "            Linear(hidden_channels, output_channels)\n",
    "        )\n",
    "        self.mlp.append(block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.mlp[i](x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_type = 'angle'\n",
    "relevance_type = 'pos'\n",
    "pos_type = \"gcn\"\n",
    "SAVE_DIR = f\"/home/ubuntu/proj/code/axolotl_softprompt/data/pubmed/{relevance_type}\"\n",
    "input_channels_node=768 if encoder_type == 'bert' else 1024\n",
    "order = 1\n",
    "train_pos_tokens = torch.load(os.path.join(SAVE_DIR, f'train_{pos_type}_order{order}-{encoder_type}.pt'))\n",
    "test_pos_tokens = torch.load(os.path.join(SAVE_DIR, f'test_{pos_type}_order{order}-{encoder_type}.pt'))\n",
    "\n",
    "y_train, y_test = graph.y[train_split], graph.y[test_split]\n",
    "num_classes = len(torch.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Create a TensorDataset\n",
    "train_dataset = TensorDataset(train_pos_tokens, y_train)\n",
    "test_dataset = TensorDataset(test_pos_tokens, y_test)\n",
    "\n",
    "# create a DataLoader for batching\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "model = MLP(\n",
    "    input_channels_node=input_channels_node, \n",
    "    hidden_channels=1024, \n",
    "    output_channels=num_classes\n",
    "    )\n",
    "device = torch.device('cuda:1')\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader):\n",
    "    loss_list = []\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        batch_x, batch_y = batch\n",
    "        batch_x = batch_x.view(-1, input_channels_node)\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        output = model(batch_x)\n",
    "        loss = criterion(output, batch_y.long())\n",
    "        loss_list.append(float(loss))\n",
    "        loss.backward()\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "    return np.mean(loss_list)\n",
    "\n",
    "def test(dataloader):\n",
    "    y_true_list, y_pred_list = [], []\n",
    "    model.eval()\n",
    "    for batch in dataloader:\n",
    "        batch_x, batch_y = batch\n",
    "        batch_x = batch_x.view(-1, input_channels_node)\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        output = model(batch_x)\n",
    "        y_pred = output.argmax(dim=-1)\n",
    "        y_true_list.append(batch_y.detach().cpu().numpy().reshape(-1))\n",
    "        y_pred_list.append(y_pred.detach().cpu().numpy().reshape(-1))\n",
    "    y_true = np.concatenate(y_true_list)\n",
    "    y_pred = np.concatenate(y_pred_list)\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=  1, loss=0.405, train_score=0.874, test_score=0.837\n",
      "epoch=  2, loss=0.336, train_score=0.887, test_score=0.850\n",
      "epoch=  3, loss=0.304, train_score=0.903, test_score=0.860\n",
      "epoch=  4, loss=0.274, train_score=0.912, test_score=0.866\n",
      "epoch=  5, loss=0.250, train_score=0.917, test_score=0.853\n",
      "epoch=  6, loss=0.222, train_score=0.931, test_score=0.861\n",
      "epoch=  7, loss=0.199, train_score=0.941, test_score=0.870\n",
      "epoch=  8, loss=0.174, train_score=0.942, test_score=0.849\n",
      "epoch=  9, loss=0.157, train_score=0.953, test_score=0.864\n",
      "epoch= 10, loss=0.138, train_score=0.957, test_score=0.846\n",
      "epoch= 11, loss=0.119, train_score=0.963, test_score=0.844\n",
      "epoch= 12, loss=0.104, train_score=0.969, test_score=0.867\n",
      "epoch= 13, loss=0.104, train_score=0.963, test_score=0.850\n",
      "epoch= 14, loss=0.096, train_score=0.976, test_score=0.852\n",
      "epoch= 15, loss=0.074, train_score=0.986, test_score=0.861\n",
      "epoch= 16, loss=0.072, train_score=0.984, test_score=0.851\n",
      "epoch= 17, loss=0.058, train_score=0.981, test_score=0.860\n",
      "epoch= 18, loss=0.063, train_score=0.984, test_score=0.848\n",
      "epoch= 19, loss=0.060, train_score=0.987, test_score=0.848\n",
      "epoch= 20, loss=0.040, train_score=0.987, test_score=0.859\n",
      "epoch= 21, loss=0.056, train_score=0.991, test_score=0.861\n",
      "epoch= 22, loss=0.045, train_score=0.984, test_score=0.835\n",
      "epoch= 23, loss=0.039, train_score=0.987, test_score=0.855\n",
      "epoch= 24, loss=0.031, train_score=0.981, test_score=0.848\n",
      "epoch= 25, loss=0.046, train_score=0.987, test_score=0.848\n",
      "epoch= 26, loss=0.038, train_score=0.987, test_score=0.850\n",
      "epoch= 27, loss=0.045, train_score=0.989, test_score=0.854\n",
      "epoch= 28, loss=0.037, train_score=0.990, test_score=0.856\n",
      "epoch= 29, loss=0.044, train_score=0.990, test_score=0.840\n",
      "epoch= 30, loss=0.028, train_score=0.993, test_score=0.846\n",
      "epoch= 31, loss=0.017, train_score=0.995, test_score=0.850\n",
      "epoch= 32, loss=0.035, train_score=0.991, test_score=0.863\n",
      "epoch= 33, loss=0.037, train_score=0.993, test_score=0.858\n",
      "epoch= 34, loss=0.022, train_score=0.996, test_score=0.854\n",
      "epoch= 35, loss=0.030, train_score=0.989, test_score=0.856\n",
      "epoch= 36, loss=0.032, train_score=0.990, test_score=0.844\n",
      "epoch= 37, loss=0.034, train_score=0.993, test_score=0.842\n",
      "epoch= 38, loss=0.020, train_score=0.996, test_score=0.856\n",
      "epoch= 39, loss=0.012, train_score=0.998, test_score=0.856\n",
      "epoch= 40, loss=0.017, train_score=0.990, test_score=0.860\n",
      "epoch= 41, loss=0.028, train_score=0.994, test_score=0.850\n",
      "epoch= 42, loss=0.024, train_score=0.988, test_score=0.851\n",
      "epoch= 43, loss=0.054, train_score=0.987, test_score=0.857\n",
      "epoch= 44, loss=0.032, train_score=0.993, test_score=0.839\n",
      "epoch= 45, loss=0.014, train_score=0.994, test_score=0.854\n",
      "epoch= 46, loss=0.025, train_score=0.990, test_score=0.854\n",
      "epoch= 47, loss=0.024, train_score=0.993, test_score=0.863\n",
      "epoch= 48, loss=0.016, train_score=0.996, test_score=0.867\n",
      "epoch= 49, loss=0.021, train_score=0.995, test_score=0.863\n",
      "epoch= 50, loss=0.024, train_score=0.996, test_score=0.858\n",
      "epoch= 51, loss=0.013, train_score=0.997, test_score=0.866\n",
      "epoch= 52, loss=0.020, train_score=0.992, test_score=0.861\n",
      "epoch= 53, loss=0.024, train_score=0.995, test_score=0.876\n",
      "epoch= 54, loss=0.027, train_score=0.989, test_score=0.840\n",
      "epoch= 55, loss=0.036, train_score=0.988, test_score=0.848\n",
      "epoch= 56, loss=0.022, train_score=0.996, test_score=0.844\n",
      "epoch= 57, loss=0.019, train_score=0.996, test_score=0.870\n",
      "epoch= 58, loss=0.015, train_score=0.998, test_score=0.860\n",
      "epoch= 59, loss=0.011, train_score=0.994, test_score=0.863\n",
      "epoch= 60, loss=0.009, train_score=0.993, test_score=0.865\n",
      "epoch= 61, loss=0.017, train_score=0.994, test_score=0.847\n",
      "epoch= 62, loss=0.034, train_score=0.993, test_score=0.855\n",
      "epoch= 63, loss=0.030, train_score=0.995, test_score=0.862\n",
      "epoch= 64, loss=0.029, train_score=0.997, test_score=0.858\n",
      "epoch= 65, loss=0.020, train_score=0.996, test_score=0.855\n",
      "epoch= 66, loss=0.006, train_score=0.999, test_score=0.863\n",
      "epoch= 67, loss=0.005, train_score=0.999, test_score=0.863\n",
      "epoch= 68, loss=0.007, train_score=0.999, test_score=0.871\n",
      "epoch= 69, loss=0.028, train_score=0.991, test_score=0.857\n",
      "epoch= 70, loss=0.035, train_score=0.991, test_score=0.851\n",
      "epoch= 71, loss=0.020, train_score=0.999, test_score=0.857\n",
      "epoch= 72, loss=0.006, train_score=0.999, test_score=0.859\n",
      "epoch= 73, loss=0.007, train_score=0.999, test_score=0.851\n",
      "epoch= 74, loss=0.011, train_score=0.994, test_score=0.854\n",
      "epoch= 75, loss=0.019, train_score=0.997, test_score=0.859\n",
      "epoch= 76, loss=0.024, train_score=0.994, test_score=0.853\n",
      "epoch= 77, loss=0.041, train_score=0.978, test_score=0.834\n",
      "epoch= 78, loss=0.026, train_score=0.996, test_score=0.858\n",
      "epoch= 79, loss=0.014, train_score=0.998, test_score=0.857\n",
      "epoch= 80, loss=0.010, train_score=0.999, test_score=0.854\n",
      "epoch= 81, loss=0.010, train_score=0.999, test_score=0.860\n",
      "epoch= 82, loss=0.007, train_score=0.998, test_score=0.854\n",
      "epoch= 83, loss=0.010, train_score=0.997, test_score=0.852\n",
      "epoch= 84, loss=0.015, train_score=0.994, test_score=0.852\n",
      "epoch= 85, loss=0.022, train_score=0.992, test_score=0.865\n",
      "epoch= 86, loss=0.020, train_score=0.995, test_score=0.847\n",
      "epoch= 87, loss=0.028, train_score=0.993, test_score=0.845\n",
      "epoch= 88, loss=0.016, train_score=0.999, test_score=0.854\n",
      "epoch= 89, loss=0.011, train_score=0.997, test_score=0.858\n",
      "epoch= 90, loss=0.020, train_score=0.996, test_score=0.849\n",
      "epoch= 91, loss=0.032, train_score=0.995, test_score=0.867\n",
      "epoch= 92, loss=0.012, train_score=0.997, test_score=0.855\n",
      "epoch= 93, loss=0.018, train_score=0.994, test_score=0.850\n",
      "epoch= 94, loss=0.023, train_score=0.995, test_score=0.857\n",
      "epoch= 95, loss=0.010, train_score=0.998, test_score=0.861\n",
      "epoch= 96, loss=0.006, train_score=0.998, test_score=0.864\n",
      "epoch= 97, loss=0.007, train_score=0.998, test_score=0.857\n",
      "epoch= 98, loss=0.016, train_score=0.996, test_score=0.859\n",
      "epoch= 99, loss=0.026, train_score=0.994, test_score=0.855\n",
      "epoch=100, loss=0.039, train_score=0.996, test_score=0.855\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "best_val_res = 0\n",
    "best_res = {}\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    loss = train(train_dataloader)\n",
    "    y_true_train, y_pred_train = test(train_dataloader)\n",
    "    y_true_test, y_pred_test = test(test_dataloader)\n",
    "    train_score = np.mean(y_true_train == y_pred_train)\n",
    "    test_score = np.mean(y_true_test == y_pred_test)\n",
    "    print(f\"{epoch=:3d}, {loss=:.3f}, {train_score=:.3f}, {test_score=:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
