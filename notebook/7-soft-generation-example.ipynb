{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/graphllm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import dataclasses\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import pathlib\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "from tqdm import tqdm\n",
    "from xopen import xopen\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "from axolotl.prompters import AlpacaPrompter\n",
    "\n",
    "from peft import TaskType\n",
    "\n",
    "from softprompt.tuner import GraphPromptTuningConfig\n",
    "from softprompt.mapping import get_peft_graph_model\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i : i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '/home/ubuntu/proj/code/axolotl_softprompt/data'\n",
    "dataset_name='pubmed'\n",
    "pos_type='textual'\n",
    "model_path = \"/home/ubuntu/proj/llm_models\"\n",
    "model_name=\"vicuna-7b-v1.5\"\n",
    "bittype=\"8bit\"\n",
    "order = 0\n",
    "pos_name = f\"{pos_type}_order{order}\"\n",
    "adapter_path = f\"/home/ubuntu/proj/code/axolotl_softprompt/scripts/{dataset_name}/{pos_type}_order_{order}/{model_name}_{bittype}_{dataset_name}_{pos_type}_order_{order}/checkpoint-1000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18717it [00:00, 294841.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# Fetch all of the prompts\n",
    "prompts = []\n",
    "answers = []\n",
    "DATAPATH = os.path.join(input_path, dataset_name)\n",
    "if 'train.jsonl' not in os.listdir(DATAPATH):\n",
    "    raise ValueError(f\"Path {DATAPATH} does not have 'train.jsonl' in folder.\")\n",
    "with xopen(os.path.join(DATAPATH, 'train.jsonl')) as fin:\n",
    "    for i,line in tqdm(enumerate(fin)):\n",
    "        input_example = json.loads(line)\n",
    "        prompt = input_example['instruction']\n",
    "        answer = input_example['output']\n",
    "        prompts.append(prompt)\n",
    "        answers.append(answer)\n",
    "# load tensor \n",
    "pos_tensor_name = f\"train_{pos_name}.pt\"\n",
    "if pos_tensor_name not in os.listdir(DATAPATH):\n",
    "    raise ValueError(f\"Path {DATAPATH} does not have {pos_tensor_name} in folder.\")\n",
    "pos_token_tensor = torch.load(os.path.join(DATAPATH, pos_tensor_name))\n",
    "\n",
    "# re-format as alpaca format\n",
    "formatted_prompts = []\n",
    "for prompt in prompts:\n",
    "    prompter = AlpacaPrompter(prompt_style=None)\n",
    "    builded_prompt = next(prompter.build_prompt(\n",
    "        instruction = prompt\n",
    "    ))\n",
    "    formatted_prompts.append(builded_prompt)\n",
    "prompts = formatted_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.50s/it]\n",
      "/home/ubuntu/anaconda3/envs/graphllm/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/graphllm/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before load adapter\n",
      "name='transform.0.weight', para=Parameter containing:\n",
      "tensor([[-0.0207,  0.0266,  0.0116,  ..., -0.0145, -0.0330,  0.0097],\n",
      "        [-0.0152,  0.0242, -0.0065,  ..., -0.0198, -0.0067, -0.0137],\n",
      "        [-0.0357,  0.0160, -0.0045,  ...,  0.0048, -0.0304, -0.0284],\n",
      "        ...,\n",
      "        [ 0.0324,  0.0155,  0.0193,  ..., -0.0200,  0.0251, -0.0324],\n",
      "        [-0.0216, -0.0345, -0.0196,  ..., -0.0062,  0.0300, -0.0304],\n",
      "        [ 0.0065,  0.0155,  0.0037,  ...,  0.0157,  0.0154,  0.0040]],\n",
      "       requires_grad=True)\n",
      "name='transform.0.bias', para=Parameter containing:\n",
      "tensor([ 0.0238,  0.0126,  0.0056,  ..., -0.0234, -0.0343, -0.0215],\n",
      "       requires_grad=True)\n",
      "name='transform.2.weight', para=Parameter containing:\n",
      "tensor([[ 1.1070e-02,  2.2004e-02, -2.9223e-02,  ..., -2.7320e-02,\n",
      "          1.5658e-02, -7.5257e-03],\n",
      "        [ 1.2907e-02,  2.8829e-02, -2.1709e-02,  ...,  1.0715e-02,\n",
      "          1.2256e-02, -1.0334e-02],\n",
      "        [-1.5875e-02, -2.6800e-05, -5.5465e-03,  ...,  9.7783e-03,\n",
      "         -2.8517e-02, -2.9245e-02],\n",
      "        ...,\n",
      "        [-2.3385e-02,  2.3722e-02,  1.6432e-03,  ...,  3.0532e-02,\n",
      "         -1.4642e-02,  2.4240e-03],\n",
      "        [-7.0995e-03,  2.2371e-02,  1.0546e-02,  ..., -1.3925e-02,\n",
      "          2.2613e-02, -1.7592e-02],\n",
      "        [ 1.8670e-02,  2.9501e-02, -8.7394e-03,  ...,  2.4439e-02,\n",
      "         -6.8555e-04, -2.1120e-02]], requires_grad=True)\n",
      "name='transform.2.bias', para=Parameter containing:\n",
      "tensor([-0.0054, -0.0028,  0.0029,  ...,  0.0017,  0.0219,  0.0308],\n",
      "       requires_grad=True)\n",
      "after load adapter\n",
      "name='transform.0.weight', para=Parameter containing:\n",
      "tensor([[ 0.0107, -0.0086,  0.0096,  ...,  0.0165,  0.0258, -0.0194],\n",
      "        [ 0.0170,  0.0090,  0.0079,  ...,  0.0160,  0.0113, -0.0380],\n",
      "        [-0.0125, -0.0118, -0.0141,  ..., -0.0277, -0.0327, -0.0050],\n",
      "        ...,\n",
      "        [ 0.0021,  0.0028, -0.0036,  ...,  0.0073, -0.0310, -0.0118],\n",
      "        [-0.0028,  0.0134, -0.0065,  ...,  0.0246,  0.0264, -0.0193],\n",
      "        [-0.0022, -0.0054,  0.0094,  ..., -0.0118, -0.0269, -0.0107]],\n",
      "       requires_grad=True)\n",
      "name='transform.0.bias', para=Parameter containing:\n",
      "tensor([-0.0026,  0.0302, -0.0346,  ..., -0.0034, -0.0364,  0.0276],\n",
      "       requires_grad=True)\n",
      "name='transform.2.weight', para=Parameter containing:\n",
      "tensor([[-5.2376e-03, -1.8049e-02,  2.2415e-03,  ...,  1.7817e-02,\n",
      "         -1.6934e-02, -8.4731e-03],\n",
      "        [ 3.1986e-03, -2.5773e-02,  2.4272e-02,  ...,  4.5233e-03,\n",
      "         -1.4692e-02, -1.7165e-02],\n",
      "        [ 2.6010e-02,  8.3233e-04, -5.6088e-03,  ..., -1.3939e-02,\n",
      "          2.2342e-02,  1.8976e-02],\n",
      "        ...,\n",
      "        [-6.4854e-05,  2.4200e-02,  1.0680e-02,  ..., -1.9256e-02,\n",
      "         -2.6183e-03, -1.8139e-02],\n",
      "        [-5.2763e-03,  6.6994e-03, -7.4545e-03,  ..., -7.8054e-03,\n",
      "         -1.6654e-02,  1.0186e-02],\n",
      "        [ 1.2903e-02,  1.9179e-02,  1.6353e-02,  ..., -1.4573e-02,\n",
      "         -9.3501e-03,  9.6378e-03]], requires_grad=True)\n",
      "name='transform.2.bias', para=Parameter containing:\n",
      "tensor([-0.0107,  0.0045,  0.0161,  ..., -0.0285, -0.0195,  0.0129],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model_name = os.path.join(model_path, model_name)\n",
    "\n",
    "# load model and tokenizer\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# build prompt tuning model\n",
    "peft_config = GraphPromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    input_embedding_dim=768,\n",
    "    num_virtual_tokens=4,\n",
    "    num_pos_tokens=1,\n",
    "    encoder_hidden_size=1024,\n",
    "    embed_projection=True\n",
    ")\n",
    "adapter_path=adapter_path\n",
    "\n",
    "# peft\n",
    "model = get_peft_graph_model(model, peft_config)\n",
    "print('before load adapter')\n",
    "for name, para in model.prompt_encoder['default'].named_parameters():\n",
    "    print(f\"{name=}, {para=}\")\n",
    "model.load_adapter(adapter_path, adapter_name='default')\n",
    "print('after load adapter')\n",
    "for name, para in model.prompt_encoder['default'].named_parameters():\n",
    "    print(f\"{name=}, {para=}\")\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "#model.half()\n",
    "model.eval()\n",
    "\n",
    "generate_kwargs = dict(\n",
    "    max_new_tokens=256, \n",
    "    do_sample=False,\n",
    "    top_p=None,\n",
    "    return_dict_in_generate=True,\n",
    "    use_cache=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = tokenizer(prompts[0], return_tensors=\"pt\", padding=True).to(model.device)\n",
    "token_tensors = pos_token_tensor[:1]#.to(torch.float16).to(model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "            prompt_tokens=token_tensors, \n",
    "            **input_tokens,\n",
    "            **generate_kwargs\n",
    "            )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### System:\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "\n",
      "\n",
      "### Instruction:\n",
      "### USER: Question: Which category from the list that the paper most likely belong to? \n",
      "\n",
      "Belows are 3 potential categories to consider:\n",
      "Category [1](Diabetes Mellitus Type 1) \n",
      "Category [2](Diabetes Mellitus Type 2) \n",
      "Category [3](Diabetes Mellitus, Experimental) \n",
      "\n",
      "Given the keywords of a research paper, identify one category from a distinct list of research topics that you predict the paper will most likely belong to.\n",
      "### ASSISTANT:\n",
      "\n",
      "### Response:\n",
      " This paper most likely belongs to the ### System: ### System: ### System ### System ### System ### System:\n",
      "Below is a\n",
      "\n",
      " \\ \\\n"
     ]
    }
   ],
   "source": [
    "for i, generated_sequence in enumerate(outputs):\n",
    "    input_ids = input_tokens[\"input_ids\"][i]\n",
    "    #print(f\"\\n\\n {generated_sequence.dtype}\\n\\n\")\n",
    "    text = tokenizer.decode(generated_sequence, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    if input_ids is None:\n",
    "        prompt_length = 0\n",
    "    else:\n",
    "        prompt_length = len(\n",
    "            tokenizer.decode(\n",
    "                input_ids,\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=True,\n",
    "            )\n",
    "        )\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    1,   835,  2184, 29901,    13, 21140,   340,   338,   385, 15278,\n",
       "          393, 16612,   263,  3414, 29889, 14350,   263,  2933,   393,  7128,\n",
       "         2486,  1614,  2167,   278,  2009, 29889,    13,    13,    13,    13,\n",
       "         2277, 29937,  2799,  4080, 29901,    13,  2277, 29937,  3148,  1001,\n",
       "        29901,   894, 29901,  8449,  7663,   515,   278,  1051,   393,   278,\n",
       "         5650,  1556,  5517,  6852,   304, 29973, 29871,    13,    13, 21140,\n",
       "         1242,   526, 29871, 29941,  7037, 13997,   304,  2050, 29901,    13,\n",
       "        10900,   518, 29896,   850, 12130,   370, 10778,   341,   514,   277,\n",
       "          375,  5167, 29871, 29896, 29897, 29871,    13, 10900,   518, 29906,\n",
       "          850, 12130,   370, 10778,   341,   514,   277,   375,  5167, 29871,\n",
       "        29906, 29897, 29871,    13, 10900,   518, 29941,   850, 12130,   370,\n",
       "        10778,   341,   514,   277,   375, 29892,  1222, 27910, 29897, 29871,\n",
       "           13,    13, 29954,  5428,   278, 29361,   310,   263,  5925,  5650,\n",
       "        29892, 12439,   697,  7663,   515,   263,  8359,  1051,   310,  5925,\n",
       "        23820,   393,   366,  8500,   278,  5650,   674,  1556,  5517,  6852,\n",
       "          304, 29889,    13,  2277, 29937,   319,  1799,  9047, 13566, 29901,\n",
       "           13,    13,  2277, 29937, 13291, 29901,    13,   910,  5650,  1556,\n",
       "         5517,  6852,   304,  4671,   370, 10778,   341,   514,   277,   375,\n",
       "        29892,  1222, 27910,     2], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_sequence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
