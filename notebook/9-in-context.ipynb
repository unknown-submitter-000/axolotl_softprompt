{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/graphllm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import dataclasses\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import pathlib\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "from tqdm import tqdm\n",
    "from xopen import xopen\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "from axolotl.prompters import AlpacaPrompter\n",
    "\n",
    "from peft import TaskType\n",
    "\n",
    "from softprompt.tuner import GraphPromptTuningConfig\n",
    "from softprompt.mapping import get_peft_graph_model\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i : i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '/home/ubuntu/proj/code/axolotl_softprompt/data'\n",
    "dataset_name='pubmed'\n",
    "pos_type='textual'\n",
    "model_path = \"/home/ubuntu/proj/llm_models\"\n",
    "model_name=\"vicuna-7b-v1.5\"\n",
    "bittype=\"8bit\"\n",
    "order = 2\n",
    "steps = 2000\n",
    "epochs = 50\n",
    "pos_name = f\"{pos_type}_order{order}\"\n",
    "adapter_path = f\"/home/ubuntu/proj/code/axolotl_softprompt/scripts/{dataset_name}/{pos_type}_order_{order}/{model_name}_{bittype}_{dataset_name}_{pos_type}_order_{order}/checkpoint-{steps}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18717it [00:00, 294028.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# Fetch all of the prompts\n",
    "prompts = []\n",
    "answers = []\n",
    "DATAPATH = os.path.join(input_path, dataset_name)\n",
    "if 'train.jsonl' not in os.listdir(DATAPATH):\n",
    "    raise ValueError(f\"Path {DATAPATH} does not have 'train.jsonl' in folder.\")\n",
    "with xopen(os.path.join(DATAPATH, 'train.jsonl')) as fin:\n",
    "    for i,line in tqdm(enumerate(fin)):\n",
    "        input_example = json.loads(line)\n",
    "        prompt = input_example['instruction']\n",
    "        answer = input_example['output']\n",
    "        prompts.append(prompt)\n",
    "        answers.append(answer)\n",
    "        \n",
    "# load tensor \n",
    "pos_tensor_name = f\"train_{pos_name}.pt\"\n",
    "if pos_tensor_name not in os.listdir(DATAPATH):\n",
    "    raise ValueError(f\"Path {DATAPATH} does not have {pos_tensor_name} in folder.\")\n",
    "pos_token_tensor = torch.load(os.path.join(DATAPATH, pos_tensor_name))\n",
    "\n",
    "# re-format as alpaca format\n",
    "formatted_prompts = []\n",
    "for prompt in prompts:\n",
    "    prompter = AlpacaPrompter(prompt_style=None)\n",
    "    builded_prompt = next(prompter.build_prompt(\n",
    "        instruction = prompt\n",
    "    ))\n",
    "    formatted_prompts.append(builded_prompt)\n",
    "prompts = formatted_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.46s/it]\n",
      "/home/ubuntu/anaconda3/envs/graphllm/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/graphllm/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before load adapter\n",
      "after load adapter\n"
     ]
    }
   ],
   "source": [
    "model_name = os.path.join(model_path, model_name)\n",
    "\n",
    "# load model and tokenizer\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto')\n",
    "\n",
    "# build prompt tuning model\n",
    "peft_config = GraphPromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    input_embedding_dim=768,\n",
    "    num_virtual_tokens=4,\n",
    "    num_pos_tokens=1,\n",
    "    encoder_hidden_size=1024,\n",
    "    embed_projection=True\n",
    ")\n",
    "adapter_path=adapter_path\n",
    "\n",
    "# peft\n",
    "model = get_peft_graph_model(model, peft_config)\n",
    "print('before load adapter')\n",
    "#for name, para in model.prompt_encoder['default'].named_parameters():\n",
    "#    print(f\"{name=}, {para=}\")\n",
    "model.load_adapter(adapter_path, adapter_name='default')\n",
    "print('after load adapter')\n",
    "#for name, para in model.prompt_encoder['default'].named_parameters():\n",
    "#    print(f\"{name=}, {para=}\")\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model.half()\n",
    "model.eval()\n",
    "\n",
    "generate_kwargs = dict(\n",
    "    max_new_tokens=256,\n",
    "    do_sample=False,\n",
    "    top_p=None,\n",
    "    return_dict_in_generate=True,\n",
    "    use_cache=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/graphllm/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/graphllm/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/proj/code/llm_long_context/src/softprompt/peft_graph_model.py:260: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n",
      "  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I think this paper most likely belong to Diabetes Mellitus, Experimental because the topic of the paper is about a new treatment for diabetes and the paper mention that the treatment is based on a new technology that can regulate the glucose levels in the body, this technology is not yet approved by the FDA, so it is considered as experimental.\n",
      "\n",
      "Please let me know if you have any other question or if you need any help.\n",
      "*******************************\n",
      "\n",
      "*******************************\n",
      "\n",
      "\n",
      "\n",
      "I think this paper most likely belong to Diabetes Mellitus, Experimental because the keywords used in the instruction \"Diabetes Mellitus, Experimental\" and the paper most likely deal with new treatment or study on Diabetes Mellitus that not yet been approved or commercialized.\n",
      "*******************************\n",
      "\n",
      "*******************************\n",
      "\n",
      "\n",
      "\n",
      "I think this paper most likely belong to Diabetes Mellitus, Experimental because the keywords given in the instruction are related to experimental research on diabetes, specifically in the context of type 2 diabetes. The paper may discuss new treatments or therapies for type 2 diabetes, or possible causes and risk factors for the disease. Additionally, the paper may explore the impact of diabetes on different organs or body functions, or the genetic and environmental factors that contribute to the development of type 2 diabetes.\n",
      "*******************************\n",
      "\n",
      "\n",
      "\n",
      "I think this paper most likely belong to Diabetes Mellitus Type 2 because the keyword used in the paper is \"Diabetes Mellitus Type 2\" and it is related to the research topic of \"Diabetes Mellitus Type 2\" which is a subcategory of Diabetes Mellitus Type 2.\n",
      "*******************************\n",
      "\n",
      "\n",
      "\n",
      "I think this paper most likely belong to Diabetes Mellitus Type 2 because the keyword used in the paper is \"Diabetes Mellitus Type 2\" and it is a specific category within the Diabetes Mellitus Type.\n",
      "*******************************\n",
      "\n",
      "*******************************\n",
      "\n",
      "*******************************\n",
      "\n",
      "\n",
      "\n",
      "I think this paper most likely belong to Diabetes Mellitus Type 1 because the keyword used in the paper is \"Diabetes Mellitus Type 1\" and it's the first keyword in the paper, it indicates that the paper is about a study or research related to Diabetes Mellitus Type 1.\n",
      "*******************************\n"
     ]
    }
   ],
   "source": [
    "for index_value in range(10):\n",
    "    input_tokens = tokenizer(prompts[index_value], return_tensors=\"pt\", padding=True).to(model.device)\n",
    "    token_tensors = pos_token_tensor[index_value:index_value+1].to(torch.float16).to(model.device)\n",
    "    input_ids = input_tokens['input_ids']\n",
    "    outputs = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                prompt_tokens=token_tensors, \n",
    "                **generate_kwargs\n",
    "                )[0]\n",
    "    for i, generated_sequence in enumerate(outputs):\n",
    "        input_ids = input_tokens[\"input_ids\"][i]\n",
    "        #print(f\"\\n\\n {generated_sequence.dtype}\\n\\n\")\n",
    "        text = tokenizer.decode(generated_sequence, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "        if input_ids is None:\n",
    "            prompt_length = 0\n",
    "        else:\n",
    "            prompt_length = len(\n",
    "                tokenizer.decode(\n",
    "                    input_ids,\n",
    "                    skip_special_tokens=True,\n",
    "                    clean_up_tokenization_spaces=True,\n",
    "                )\n",
    "            )\n",
    "        #print('groundtruth:', answers[index_value],'prediction: ', text[prompt_length:])\n",
    "    used_prompt = text + \" Explain why you think this is the choice.\"\n",
    "    input_tokens = tokenizer(used_prompt, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "    token_tensors = pos_token_tensor[index_value:index_value+1].to(torch.float16).to(model.device)\n",
    "    input_ids = input_tokens['input_ids']\n",
    "    outputs = model.generate(\n",
    "                input_ids=input_ids\n",
    "                prompt_tokens=token_tensors, \n",
    "                **generate_kwargs\n",
    "                )[0]\n",
    "    for i, generated_sequence in enumerate(outputs):\n",
    "        input_ids = input_tokens[\"input_ids\"][i]\n",
    "        #print(f\"\\n\\n {generated_sequence.dtype}\\n\\n\")\n",
    "        text = tokenizer.decode(generated_sequence, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "        if input_ids is None:\n",
    "            prompt_length = 0\n",
    "        else:\n",
    "            prompt_length = len(\n",
    "                tokenizer.decode(\n",
    "                    input_ids,\n",
    "                    skip_special_tokens=True,\n",
    "                    clean_up_tokenization_spaces=True,\n",
    "                )\n",
    "            )\n",
    "        print(text[prompt_length:])\n",
    "        print(\"*******************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
