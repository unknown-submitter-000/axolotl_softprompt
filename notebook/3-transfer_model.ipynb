{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/zheng_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/envs/zheng_env/lib/python3.11/site-packages/torch_geometric/typing.py:47: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /lib64/libm.so.6: version `GLIBC_2.29' not found (required by /opt/conda/envs/zheng_env/lib/python3.11/site-packages/libpyg.so)\n",
      "  warnings.warn(f\"An issue occurred while importing 'pyg-lib'. \"\n",
      "/opt/conda/envs/zheng_env/lib/python3.11/site-packages/torch_geometric/typing.py:90: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: /lib64/libm.so.6: version `GLIBC_2.27' not found (required by /opt/conda/envs/zheng_env/lib/python3.11/site-packages/torch_spline_conv/_basis_cuda.so)\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/zheng_env/lib/python3.11/site-packages/torch_geometric/typing.py:101: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /lib64/libm.so.6: version `GLIBC_2.29' not found (required by /opt/conda/envs/zheng_env/lib/python3.11/site-packages/torch_sparse/_metis_cuda.so)\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "import os \n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pandas as pd\n",
    "from torch_geometric.data import Data\n",
    "sys.path.append(\"/home/ec2-user/proj/code/graphbert/src\")\n",
    "\n",
    "from utility.prompting import (\n",
    "    Item,\n",
    "    get_prompt_tuning_prompt\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tuner import GraphPeftType, GraphPromptTuningConfig\n",
    "from peft import TaskType\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, default_data_collator, get_linear_schedule_with_warmup\n",
    "from mapping import get_peft_graph_model\n",
    "# load a pretrained gpt-2 model\n",
    "model_name_or_path = \"/home/ec2-user/proj/llm_models/vicuna-7b-v1.5\"\n",
    "tokenizer_name_or_path = \"/home/ec2-user/proj/llm_models/vicuna-7b-v1.5\"\n",
    "peft_config = GraphPromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    input_embedding_dim=768,\n",
    "    num_virtual_tokens=4,\n",
    "    encoder_hidden_size=1024,\n",
    "    embed_projection=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.51s/it]\n",
      "/opt/conda/envs/zheng_env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/zheng_env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 17,581,056 || all params: 6,755,996,672 || trainable%: 0.26022890261127707\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "graph_prompt_model = get_peft_graph_model(model, peft_config)\n",
    "print(graph_prompt_model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0200,  0.0046, -0.0208,  ..., -0.0274, -0.0224,  0.0160],\n",
       "        [ 0.0316,  0.0214, -0.0300,  ..., -0.0263,  0.0119,  0.0053],\n",
       "        [ 0.0349,  0.0080, -0.0024,  ..., -0.0080,  0.0105,  0.0010],\n",
       "        ...,\n",
       "        [-0.0105,  0.0175,  0.0234,  ..., -0.0070, -0.0318,  0.0046],\n",
       "        [ 0.0006,  0.0028, -0.0201,  ..., -0.0029,  0.0320, -0.0133],\n",
       "        [-0.0058,  0.0061, -0.0187,  ..., -0.0026, -0.0347, -0.0049]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_prompt_model.prompt_encoder['default'].transform[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['base_model.model.embed_tokens.weight', 'base_model.model.layers.0.self_attn.q_proj.weight', 'base_model.model.layers.0.self_attn.k_proj.weight', 'base_model.model.layers.0.self_attn.v_proj.weight', 'base_model.model.layers.0.self_attn.o_proj.weight', 'base_model.model.layers.0.mlp.gate_proj.weight', 'base_model.model.layers.0.mlp.up_proj.weight', 'base_model.model.layers.0.mlp.down_proj.weight', 'base_model.model.layers.0.input_layernorm.weight', 'base_model.model.layers.0.post_attention_layernorm.weight', 'base_model.model.layers.1.self_attn.q_proj.weight', 'base_model.model.layers.1.self_attn.k_proj.weight', 'base_model.model.layers.1.self_attn.v_proj.weight', 'base_model.model.layers.1.self_attn.o_proj.weight', 'base_model.model.layers.1.mlp.gate_proj.weight', 'base_model.model.layers.1.mlp.up_proj.weight', 'base_model.model.layers.1.mlp.down_proj.weight', 'base_model.model.layers.1.input_layernorm.weight', 'base_model.model.layers.1.post_attention_layernorm.weight', 'base_model.model.layers.2.self_attn.q_proj.weight', 'base_model.model.layers.2.self_attn.k_proj.weight', 'base_model.model.layers.2.self_attn.v_proj.weight', 'base_model.model.layers.2.self_attn.o_proj.weight', 'base_model.model.layers.2.mlp.gate_proj.weight', 'base_model.model.layers.2.mlp.up_proj.weight', 'base_model.model.layers.2.mlp.down_proj.weight', 'base_model.model.layers.2.input_layernorm.weight', 'base_model.model.layers.2.post_attention_layernorm.weight', 'base_model.model.layers.3.self_attn.q_proj.weight', 'base_model.model.layers.3.self_attn.k_proj.weight', 'base_model.model.layers.3.self_attn.v_proj.weight', 'base_model.model.layers.3.self_attn.o_proj.weight', 'base_model.model.layers.3.mlp.gate_proj.weight', 'base_model.model.layers.3.mlp.up_proj.weight', 'base_model.model.layers.3.mlp.down_proj.weight', 'base_model.model.layers.3.input_layernorm.weight', 'base_model.model.layers.3.post_attention_layernorm.weight', 'base_model.model.layers.4.self_attn.q_proj.weight', 'base_model.model.layers.4.self_attn.k_proj.weight', 'base_model.model.layers.4.self_attn.v_proj.weight', 'base_model.model.layers.4.self_attn.o_proj.weight', 'base_model.model.layers.4.mlp.gate_proj.weight', 'base_model.model.layers.4.mlp.up_proj.weight', 'base_model.model.layers.4.mlp.down_proj.weight', 'base_model.model.layers.4.input_layernorm.weight', 'base_model.model.layers.4.post_attention_layernorm.weight', 'base_model.model.layers.5.self_attn.q_proj.weight', 'base_model.model.layers.5.self_attn.k_proj.weight', 'base_model.model.layers.5.self_attn.v_proj.weight', 'base_model.model.layers.5.self_attn.o_proj.weight', 'base_model.model.layers.5.mlp.gate_proj.weight', 'base_model.model.layers.5.mlp.up_proj.weight', 'base_model.model.layers.5.mlp.down_proj.weight', 'base_model.model.layers.5.input_layernorm.weight', 'base_model.model.layers.5.post_attention_layernorm.weight', 'base_model.model.layers.6.self_attn.q_proj.weight', 'base_model.model.layers.6.self_attn.k_proj.weight', 'base_model.model.layers.6.self_attn.v_proj.weight', 'base_model.model.layers.6.self_attn.o_proj.weight', 'base_model.model.layers.6.mlp.gate_proj.weight', 'base_model.model.layers.6.mlp.up_proj.weight', 'base_model.model.layers.6.mlp.down_proj.weight', 'base_model.model.layers.6.input_layernorm.weight', 'base_model.model.layers.6.post_attention_layernorm.weight', 'base_model.model.layers.7.self_attn.q_proj.weight', 'base_model.model.layers.7.self_attn.k_proj.weight', 'base_model.model.layers.7.self_attn.v_proj.weight', 'base_model.model.layers.7.self_attn.o_proj.weight', 'base_model.model.layers.7.mlp.gate_proj.weight', 'base_model.model.layers.7.mlp.up_proj.weight', 'base_model.model.layers.7.mlp.down_proj.weight', 'base_model.model.layers.7.input_layernorm.weight', 'base_model.model.layers.7.post_attention_layernorm.weight', 'base_model.model.layers.8.self_attn.q_proj.weight', 'base_model.model.layers.8.self_attn.k_proj.weight', 'base_model.model.layers.8.self_attn.v_proj.weight', 'base_model.model.layers.8.self_attn.o_proj.weight', 'base_model.model.layers.8.mlp.gate_proj.weight', 'base_model.model.layers.8.mlp.up_proj.weight', 'base_model.model.layers.8.mlp.down_proj.weight', 'base_model.model.layers.8.input_layernorm.weight', 'base_model.model.layers.8.post_attention_layernorm.weight', 'base_model.model.layers.9.self_attn.q_proj.weight', 'base_model.model.layers.9.self_attn.k_proj.weight', 'base_model.model.layers.9.self_attn.v_proj.weight', 'base_model.model.layers.9.self_attn.o_proj.weight', 'base_model.model.layers.9.mlp.gate_proj.weight', 'base_model.model.layers.9.mlp.up_proj.weight', 'base_model.model.layers.9.mlp.down_proj.weight', 'base_model.model.layers.9.input_layernorm.weight', 'base_model.model.layers.9.post_attention_layernorm.weight', 'base_model.model.layers.10.self_attn.q_proj.weight', 'base_model.model.layers.10.self_attn.k_proj.weight', 'base_model.model.layers.10.self_attn.v_proj.weight', 'base_model.model.layers.10.self_attn.o_proj.weight', 'base_model.model.layers.10.mlp.gate_proj.weight', 'base_model.model.layers.10.mlp.up_proj.weight', 'base_model.model.layers.10.mlp.down_proj.weight', 'base_model.model.layers.10.input_layernorm.weight', 'base_model.model.layers.10.post_attention_layernorm.weight', 'base_model.model.layers.11.self_attn.q_proj.weight', 'base_model.model.layers.11.self_attn.k_proj.weight', 'base_model.model.layers.11.self_attn.v_proj.weight', 'base_model.model.layers.11.self_attn.o_proj.weight', 'base_model.model.layers.11.mlp.gate_proj.weight', 'base_model.model.layers.11.mlp.up_proj.weight', 'base_model.model.layers.11.mlp.down_proj.weight', 'base_model.model.layers.11.input_layernorm.weight', 'base_model.model.layers.11.post_attention_layernorm.weight', 'base_model.model.layers.12.self_attn.q_proj.weight', 'base_model.model.layers.12.self_attn.k_proj.weight', 'base_model.model.layers.12.self_attn.v_proj.weight', 'base_model.model.layers.12.self_attn.o_proj.weight', 'base_model.model.layers.12.mlp.gate_proj.weight', 'base_model.model.layers.12.mlp.up_proj.weight', 'base_model.model.layers.12.mlp.down_proj.weight', 'base_model.model.layers.12.input_layernorm.weight', 'base_model.model.layers.12.post_attention_layernorm.weight', 'base_model.model.layers.13.self_attn.q_proj.weight', 'base_model.model.layers.13.self_attn.k_proj.weight', 'base_model.model.layers.13.self_attn.v_proj.weight', 'base_model.model.layers.13.self_attn.o_proj.weight', 'base_model.model.layers.13.mlp.gate_proj.weight', 'base_model.model.layers.13.mlp.up_proj.weight', 'base_model.model.layers.13.mlp.down_proj.weight', 'base_model.model.layers.13.input_layernorm.weight', 'base_model.model.layers.13.post_attention_layernorm.weight', 'base_model.model.layers.14.self_attn.q_proj.weight', 'base_model.model.layers.14.self_attn.k_proj.weight', 'base_model.model.layers.14.self_attn.v_proj.weight', 'base_model.model.layers.14.self_attn.o_proj.weight', 'base_model.model.layers.14.mlp.gate_proj.weight', 'base_model.model.layers.14.mlp.up_proj.weight', 'base_model.model.layers.14.mlp.down_proj.weight', 'base_model.model.layers.14.input_layernorm.weight', 'base_model.model.layers.14.post_attention_layernorm.weight', 'base_model.model.layers.15.self_attn.q_proj.weight', 'base_model.model.layers.15.self_attn.k_proj.weight', 'base_model.model.layers.15.self_attn.v_proj.weight', 'base_model.model.layers.15.self_attn.o_proj.weight', 'base_model.model.layers.15.mlp.gate_proj.weight', 'base_model.model.layers.15.mlp.up_proj.weight', 'base_model.model.layers.15.mlp.down_proj.weight', 'base_model.model.layers.15.input_layernorm.weight', 'base_model.model.layers.15.post_attention_layernorm.weight', 'base_model.model.layers.16.self_attn.q_proj.weight', 'base_model.model.layers.16.self_attn.k_proj.weight', 'base_model.model.layers.16.self_attn.v_proj.weight', 'base_model.model.layers.16.self_attn.o_proj.weight', 'base_model.model.layers.16.mlp.gate_proj.weight', 'base_model.model.layers.16.mlp.up_proj.weight', 'base_model.model.layers.16.mlp.down_proj.weight', 'base_model.model.layers.16.input_layernorm.weight', 'base_model.model.layers.16.post_attention_layernorm.weight', 'base_model.model.layers.17.self_attn.q_proj.weight', 'base_model.model.layers.17.self_attn.k_proj.weight', 'base_model.model.layers.17.self_attn.v_proj.weight', 'base_model.model.layers.17.self_attn.o_proj.weight', 'base_model.model.layers.17.mlp.gate_proj.weight', 'base_model.model.layers.17.mlp.up_proj.weight', 'base_model.model.layers.17.mlp.down_proj.weight', 'base_model.model.layers.17.input_layernorm.weight', 'base_model.model.layers.17.post_attention_layernorm.weight', 'base_model.model.layers.18.self_attn.q_proj.weight', 'base_model.model.layers.18.self_attn.k_proj.weight', 'base_model.model.layers.18.self_attn.v_proj.weight', 'base_model.model.layers.18.self_attn.o_proj.weight', 'base_model.model.layers.18.mlp.gate_proj.weight', 'base_model.model.layers.18.mlp.up_proj.weight', 'base_model.model.layers.18.mlp.down_proj.weight', 'base_model.model.layers.18.input_layernorm.weight', 'base_model.model.layers.18.post_attention_layernorm.weight', 'base_model.model.layers.19.self_attn.q_proj.weight', 'base_model.model.layers.19.self_attn.k_proj.weight', 'base_model.model.layers.19.self_attn.v_proj.weight', 'base_model.model.layers.19.self_attn.o_proj.weight', 'base_model.model.layers.19.mlp.gate_proj.weight', 'base_model.model.layers.19.mlp.up_proj.weight', 'base_model.model.layers.19.mlp.down_proj.weight', 'base_model.model.layers.19.input_layernorm.weight', 'base_model.model.layers.19.post_attention_layernorm.weight', 'base_model.model.layers.20.self_attn.q_proj.weight', 'base_model.model.layers.20.self_attn.k_proj.weight', 'base_model.model.layers.20.self_attn.v_proj.weight', 'base_model.model.layers.20.self_attn.o_proj.weight', 'base_model.model.layers.20.mlp.gate_proj.weight', 'base_model.model.layers.20.mlp.up_proj.weight', 'base_model.model.layers.20.mlp.down_proj.weight', 'base_model.model.layers.20.input_layernorm.weight', 'base_model.model.layers.20.post_attention_layernorm.weight', 'base_model.model.layers.21.self_attn.q_proj.weight', 'base_model.model.layers.21.self_attn.k_proj.weight', 'base_model.model.layers.21.self_attn.v_proj.weight', 'base_model.model.layers.21.self_attn.o_proj.weight', 'base_model.model.layers.21.mlp.gate_proj.weight', 'base_model.model.layers.21.mlp.up_proj.weight', 'base_model.model.layers.21.mlp.down_proj.weight', 'base_model.model.layers.21.input_layernorm.weight', 'base_model.model.layers.21.post_attention_layernorm.weight', 'base_model.model.layers.22.self_attn.q_proj.weight', 'base_model.model.layers.22.self_attn.k_proj.weight', 'base_model.model.layers.22.self_attn.v_proj.weight', 'base_model.model.layers.22.self_attn.o_proj.weight', 'base_model.model.layers.22.mlp.gate_proj.weight', 'base_model.model.layers.22.mlp.up_proj.weight', 'base_model.model.layers.22.mlp.down_proj.weight', 'base_model.model.layers.22.input_layernorm.weight', 'base_model.model.layers.22.post_attention_layernorm.weight', 'base_model.model.layers.23.self_attn.q_proj.weight', 'base_model.model.layers.23.self_attn.k_proj.weight', 'base_model.model.layers.23.self_attn.v_proj.weight', 'base_model.model.layers.23.self_attn.o_proj.weight', 'base_model.model.layers.23.mlp.gate_proj.weight', 'base_model.model.layers.23.mlp.up_proj.weight', 'base_model.model.layers.23.mlp.down_proj.weight', 'base_model.model.layers.23.input_layernorm.weight', 'base_model.model.layers.23.post_attention_layernorm.weight', 'base_model.model.layers.24.self_attn.q_proj.weight', 'base_model.model.layers.24.self_attn.k_proj.weight', 'base_model.model.layers.24.self_attn.v_proj.weight', 'base_model.model.layers.24.self_attn.o_proj.weight', 'base_model.model.layers.24.mlp.gate_proj.weight', 'base_model.model.layers.24.mlp.up_proj.weight', 'base_model.model.layers.24.mlp.down_proj.weight', 'base_model.model.layers.24.input_layernorm.weight', 'base_model.model.layers.24.post_attention_layernorm.weight', 'base_model.model.layers.25.self_attn.q_proj.weight', 'base_model.model.layers.25.self_attn.k_proj.weight', 'base_model.model.layers.25.self_attn.v_proj.weight', 'base_model.model.layers.25.self_attn.o_proj.weight', 'base_model.model.layers.25.mlp.gate_proj.weight', 'base_model.model.layers.25.mlp.up_proj.weight', 'base_model.model.layers.25.mlp.down_proj.weight', 'base_model.model.layers.25.input_layernorm.weight', 'base_model.model.layers.25.post_attention_layernorm.weight', 'base_model.model.layers.26.self_attn.q_proj.weight', 'base_model.model.layers.26.self_attn.k_proj.weight', 'base_model.model.layers.26.self_attn.v_proj.weight', 'base_model.model.layers.26.self_attn.o_proj.weight', 'base_model.model.layers.26.mlp.gate_proj.weight', 'base_model.model.layers.26.mlp.up_proj.weight', 'base_model.model.layers.26.mlp.down_proj.weight', 'base_model.model.layers.26.input_layernorm.weight', 'base_model.model.layers.26.post_attention_layernorm.weight', 'base_model.model.layers.27.self_attn.q_proj.weight', 'base_model.model.layers.27.self_attn.k_proj.weight', 'base_model.model.layers.27.self_attn.v_proj.weight', 'base_model.model.layers.27.self_attn.o_proj.weight', 'base_model.model.layers.27.mlp.gate_proj.weight', 'base_model.model.layers.27.mlp.up_proj.weight', 'base_model.model.layers.27.mlp.down_proj.weight', 'base_model.model.layers.27.input_layernorm.weight', 'base_model.model.layers.27.post_attention_layernorm.weight', 'base_model.model.layers.28.self_attn.q_proj.weight', 'base_model.model.layers.28.self_attn.k_proj.weight', 'base_model.model.layers.28.self_attn.v_proj.weight', 'base_model.model.layers.28.self_attn.o_proj.weight', 'base_model.model.layers.28.mlp.gate_proj.weight', 'base_model.model.layers.28.mlp.up_proj.weight', 'base_model.model.layers.28.mlp.down_proj.weight', 'base_model.model.layers.28.input_layernorm.weight', 'base_model.model.layers.28.post_attention_layernorm.weight', 'base_model.model.layers.29.self_attn.q_proj.weight', 'base_model.model.layers.29.self_attn.k_proj.weight', 'base_model.model.layers.29.self_attn.v_proj.weight', 'base_model.model.layers.29.self_attn.o_proj.weight', 'base_model.model.layers.29.mlp.gate_proj.weight', 'base_model.model.layers.29.mlp.up_proj.weight', 'base_model.model.layers.29.mlp.down_proj.weight', 'base_model.model.layers.29.input_layernorm.weight', 'base_model.model.layers.29.post_attention_layernorm.weight', 'base_model.model.layers.30.self_attn.q_proj.weight', 'base_model.model.layers.30.self_attn.k_proj.weight', 'base_model.model.layers.30.self_attn.v_proj.weight', 'base_model.model.layers.30.self_attn.o_proj.weight', 'base_model.model.layers.30.mlp.gate_proj.weight', 'base_model.model.layers.30.mlp.up_proj.weight', 'base_model.model.layers.30.mlp.down_proj.weight', 'base_model.model.layers.30.input_layernorm.weight', 'base_model.model.layers.30.post_attention_layernorm.weight', 'base_model.model.layers.31.self_attn.q_proj.weight', 'base_model.model.layers.31.self_attn.k_proj.weight', 'base_model.model.layers.31.self_attn.v_proj.weight', 'base_model.model.layers.31.self_attn.o_proj.weight', 'base_model.model.layers.31.mlp.gate_proj.weight', 'base_model.model.layers.31.mlp.up_proj.weight', 'base_model.model.layers.31.mlp.down_proj.weight', 'base_model.model.layers.31.input_layernorm.weight', 'base_model.model.layers.31.post_attention_layernorm.weight', 'base_model.model.norm.weight', 'base_model.lm_head.weight', 'word_embeddings.weight'], unexpected_keys=[])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model_id = '/home/ec2-user/proj/code/graphbert/saved_models/arxiv-vicuna-7b-v1.5'\n",
    "graph_prompt_model.load_adapter(peft_model_id, adapter_name='default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0034,  0.0167,  0.0274,  ..., -0.0363, -0.0467, -0.0033],\n",
       "        [-0.0399, -0.0430,  0.0293,  ..., -0.0030,  0.0526, -0.0337],\n",
       "        [-0.0327,  0.0273, -0.0103,  ..., -0.0046,  0.0975,  0.0366],\n",
       "        ...,\n",
       "        [ 0.0271, -0.0375,  0.0170,  ...,  0.0162,  0.0319, -0.0098],\n",
       "        [ 0.0135,  0.0292, -0.0323,  ..., -0.0199, -0.0278, -0.0274],\n",
       "        [-0.0347, -0.0009, -0.0374,  ..., -0.0263, -0.0378,  0.0112]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_prompt_model.prompt_encoder['default'].transform[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/zheng_env/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from datasets import load_from_disk\n",
    "\n",
    "PREPARED_DATASET_PATH = \"/home/ec2-user/proj/code/graphbert/last-run-prepared/text_graph_arxiv\"\n",
    "\n",
    "# Load the processed training dataset\n",
    "processed_datasets = load_from_disk(f'{PREPARED_DATASET_PATH}/train_dataset')\n",
    "\n",
    "# Load the processed test dataset\n",
    "processed_datasets_test = load_from_disk(f'{PREPARED_DATASET_PATH}/test_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['embeds', 'labels', 'input_ids', 'attention_mask', 'prompt_tokens'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_datasets['train'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/ec2-user/proj/code/graphbert/src\")\n",
    "from utility.prompting import (\n",
    "    Item,\n",
    "    get_prompt_tuning_prompt\n",
    ")\n",
    "\n",
    "PROMPT_SETTINGS = {\n",
    "    'arxiv':{\n",
    "        'desc': \"Question: Which category from the list that the paper most likely belong to?\",\n",
    "        'categories': ['Artificial Intelligence', 'Computation and Language',\n",
    "                        'Computational Complexity',\n",
    "                        'Computational Engineering, Finance, and Science',\n",
    "                        'Computational Geometry', 'Computer Science and Game Theory',\n",
    "                        'Computer Vision and Pattern Recognition', 'Computers and Society',\n",
    "                        'Cryptography and Security', 'Data Structures and Algorithms',\n",
    "                        'Databases', 'Digital Libraries', 'Discrete Mathematics',\n",
    "                        'Distributed, Parallel, and Cluster Computing',\n",
    "                        'Emerging Technologies', 'Formal Languages and Automata Theory',\n",
    "                        'General Literature', 'Graphics', 'Hardware Architecture',\n",
    "                        'Human-Computer Interaction', 'Information Retrieval',\n",
    "                        'Information Theory', 'Logic in Computer Science',\n",
    "                        'Machine Learning', 'Mathematical Software', 'Multiagent Systems',\n",
    "                        'Multimedia', 'Networking and Internet Architecture',\n",
    "                        'Neural and Evolutionary Computing', 'Numerical Analysis',\n",
    "                        'Operating Systems', 'Other Computer Science', 'Performance',\n",
    "                        'Programming Languages', 'Robotics',\n",
    "                        'Social and Information Networks', 'Software Engineering', 'Sound',\n",
    "                        'Symbolic Computation', 'Systems and Control'],\n",
    "        'question': \"Given the title and abstract of a research paper, identify one category from a distinct list of research topics that you predict the paper will most likely belong to.\"\n",
    "    },\n",
    "    'pubmed':{\n",
    "        'desc': \"Question: Which category from the list that the paper most likely belong to?\",\n",
    "        'categories': ['Diabetes Mellitus Type 1', 'Diabetes Mellitus Type 2','Diabetes Mellitus, Experimental'],\n",
    "        'question': \"Given the keywords of a research paper, identify one category from a distinct list of research topics that you predict the paper will most likely belong to.\"\n",
    "    },\n",
    "    'aids':{\n",
    "        'desc': \"Question: Which category from the list that the input molecule most likely belong to?\",\n",
    "        'categories': ['HIV antiviral active compound', 'HIV antiviral inactive compound'],\n",
    "        'question': \"Given the atoms type and their connection structure of a compound, identify if the given compound is HIV antiviral active or not.\"\n",
    "    },\n",
    "}\n",
    "task_name = 'prompt_tuning'\n",
    "DATA_NAME = \"text_graph_arxiv\"\n",
    "if 'pubmed' in DATA_NAME:\n",
    "    PROMPT_SETTINGS_DICT = PROMPT_SETTINGS['pubmed']\n",
    "elif 'aids' in DATA_NAME:\n",
    "    PROMPT_SETTINGS_DICT = PROMPT_SETTINGS['aids']\n",
    "elif 'arxiv' in DATA_NAME:\n",
    "    PROMPT_SETTINGS_DICT = PROMPT_SETTINGS['arxiv']\n",
    "else:\n",
    "    raise ValueError(f'Currently does not support {DATA_NAME}')\n",
    "desc, categories, question = PROMPT_SETTINGS_DICT['desc'], PROMPT_SETTINGS_DICT['categories'], PROMPT_SETTINGS_DICT['question']\n",
    "input_item = Item(\n",
    "    desc = desc,\n",
    "    categories = categories,\n",
    "    question = question\n",
    "    )\n",
    "hard_prompt = get_prompt_tuning_prompt(\n",
    "    task_name = task_name,\n",
    "    task_item = input_item\n",
    ")\n",
    "inputs = tokenizer(hard_prompt,return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### USER: Question: Which category from the list that the paper most likely belong to? \\n\\nBelows are 40 potential categories to consider:\\nCategory [1](Artificial Intelligence) \\nCategory [2](Computation and Language) \\nCategory [3](Computational Complexity) \\nCategory [4](Computational Engineering, Finance, and Science) \\nCategory [5](Computational Geometry) \\nCategory [6](Computer Science and Game Theory) \\nCategory [7](Computer Vision and Pattern Recognition) \\nCategory [8](Computers and Society) \\nCategory [9](Cryptography and Security) \\nCategory [10](Data Structures and Algorithms) \\nCategory [11](Databases) \\nCategory [12](Digital Libraries) \\nCategory [13](Discrete Mathematics) \\nCategory [14](Distributed, Parallel, and Cluster Computing) \\nCategory [15](Emerging Technologies) \\nCategory [16](Formal Languages and Automata Theory) \\nCategory [17](General Literature) \\nCategory [18](Graphics) \\nCategory [19](Hardware Architecture) \\nCategory [20](Human-Computer Interaction) \\nCategory [21](Information Retrieval) \\nCategory [22](Information Theory) \\nCategory [23](Logic in Computer Science) \\nCategory [24](Machine Learning) \\nCategory [25](Mathematical Software) \\nCategory [26](Multiagent Systems) \\nCategory [27](Multimedia) \\nCategory [28](Networking and Internet Architecture) \\nCategory [29](Neural and Evolutionary Computing) \\nCategory [30](Numerical Analysis) \\nCategory [31](Operating Systems) \\nCategory [32](Other Computer Science) \\nCategory [33](Performance) \\nCategory [34](Programming Languages) \\nCategory [35](Robotics) \\nCategory [36](Social and Information Networks) \\nCategory [37](Software Engineering) \\nCategory [38](Sound) \\nCategory [39](Symbolic Computation) \\nCategory [40](Systems and Control) \\n\\nGiven the title and abstract of a research paper, identify one category from a distinct list of research topics that you predict the paper will most likely belong to.\\n### ASSISTANT:'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hard_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zheng_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
